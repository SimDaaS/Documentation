{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Welcome the the documentation! Here you'll find an index for the documentation. Scenario Generator mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Sensor Data Generator mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#home","text":"Welcome the the documentation! Here you'll find an index for the documentation.","title":"Home"},{"location":"#scenario-generator","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Scenario Generator"},{"location":"#sensor-data-generator","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Sensor Data Generator"},{"location":"BoundingBoxSensor/","text":"Module Name Bounding Box Sensor Version Developer 1. Summary The Bounding Box Sensor is used to show bounding boxes (2D and 3D) around the selected class of objects presnet in the frame. This sensor first calculates the bounding boxes and then creates a bounding box around each objects on the basis of taking distance and class of object (vehicle, pedestrains, traffic signals, roadside objects, and all of these), alongwith extracting bounding box information from the selected object classes. Extracted information include: frame number, actor ID, class type, center, extends, and distance from the sensor. Contains two clickable options: 2D Boxes: Allows two dimeniosnal bounding boxes (rectangular boxes) around the objects 3D Boxes: Allows three dimeniosnal bounding boxes (cuboidial boxes) around the objects Other Customisable Parameters: Image Dimensions (width x height) Output Location 2. Method The sensor first requires inputs of whether the bounds need to be displayed in 2D boxes or 3D boxes and the maximum distance till which the objects are to be considered drawing bounding boxes. First, a mask corresponmding to each of the 29 classes are created {classes are defined in Annotator.cpp }, and these are then masked to either \"true\" and \"false\" for objects of importance interms of bounding box detections (classes such as Boundary Fencing, Bustand, Tunnels, etc are excluded, i.e. provided a \"false\" mask as these posses not much significance in terms of bounding boxes on one hand while on the other and classes such as vehicles, pedestrians, etc are masked as \"true\"). Then, each of the actors belonging to the true masks are fetched for their class information and their box extents are calculated. Data Flow Diagram: Bounding Box Sensor attributes: Input Attributes Type Default Value Description Distance float 5000 cm Maximum distance to the objects to be detected by bounding boxes bIncludeVehicle boolean false Boolean that takes input whether the vehicles are to be considered as actors surrounded by bounding boxes bIncludePedestrians boolean false Boolean that takes input whether the pedestrians are to be considered as actors surrounded by bounding boxes bIncludeTrafficSignals boolean false Boolean that takes input whether the traffic sign board/signals are to be considered as actors surrounded by bounding boxes bIncludeRoadsideObjects boolean false Boolean that takes input whether the roadside objects such as traffic cones are to be considered as actors surrounded by bounding boxes bIncludeAll boolean true Boolean that makes sure all actors are to be considered for calculating the bounding boxes Output Attributes Type Description Id int Unique identifier of the detected actor OutBox.Min FVector Coordinates of the top left corner of the bounding box OutBox.Max FVector Coordinates of the bottom left corner of the bounding box Extends.X FVector X coordinate of the bounding box around an actor Extends.Y FVector Y coordinate of the bounding box around an actor Extends.Z FVector Z coordinate of the bounding box around an actor Distance float Sensor distance from the actor Output: The box bounds of the detected actor, [x,y,z] coordinates, class ID, distance and whether the detected objects are commpletely inside or outside of the sensor view. 3. Requirements Primary requiremnent: Annotator.h . Calculating the bounding boxes is done by the file \"BoundingBoxSensor.cpp\", but for visualization (creating boxes over the detected objects in a RGB image) the following files are required: 1. SceneCaptureCamera.cpp 2. SceneCaptureCamera.h While the BoundingBoxSensor itself calculates the bounding box extents, the visulaization of the boxes is performed by the SceneCaptureSensor that uses CanvasDrawItem method to draw colored lines corresponding to the class of each object using the box extents generated by the BoundingBoxSensor.","title":"Bounding Box Sensor"},{"location":"BoundingBoxSensor/#module-name","text":"Bounding Box Sensor","title":"Module Name"},{"location":"BoundingBoxSensor/#version","text":"","title":"Version"},{"location":"BoundingBoxSensor/#developer","text":"","title":"Developer"},{"location":"BoundingBoxSensor/#1-summary","text":"The Bounding Box Sensor is used to show bounding boxes (2D and 3D) around the selected class of objects presnet in the frame. This sensor first calculates the bounding boxes and then creates a bounding box around each objects on the basis of taking distance and class of object (vehicle, pedestrains, traffic signals, roadside objects, and all of these), alongwith extracting bounding box information from the selected object classes. Extracted information include: frame number, actor ID, class type, center, extends, and distance from the sensor. Contains two clickable options: 2D Boxes: Allows two dimeniosnal bounding boxes (rectangular boxes) around the objects 3D Boxes: Allows three dimeniosnal bounding boxes (cuboidial boxes) around the objects Other Customisable Parameters: Image Dimensions (width x height) Output Location","title":"1. Summary"},{"location":"BoundingBoxSensor/#2-method","text":"The sensor first requires inputs of whether the bounds need to be displayed in 2D boxes or 3D boxes and the maximum distance till which the objects are to be considered drawing bounding boxes. First, a mask corresponmding to each of the 29 classes are created {classes are defined in Annotator.cpp }, and these are then masked to either \"true\" and \"false\" for objects of importance interms of bounding box detections (classes such as Boundary Fencing, Bustand, Tunnels, etc are excluded, i.e. provided a \"false\" mask as these posses not much significance in terms of bounding boxes on one hand while on the other and classes such as vehicles, pedestrians, etc are masked as \"true\"). Then, each of the actors belonging to the true masks are fetched for their class information and their box extents are calculated.","title":"2. Method"},{"location":"BoundingBoxSensor/#data-flow-diagram","text":"","title":"Data Flow Diagram:"},{"location":"BoundingBoxSensor/#bounding-box-sensor-attributes","text":"Input Attributes Type Default Value Description Distance float 5000 cm Maximum distance to the objects to be detected by bounding boxes bIncludeVehicle boolean false Boolean that takes input whether the vehicles are to be considered as actors surrounded by bounding boxes bIncludePedestrians boolean false Boolean that takes input whether the pedestrians are to be considered as actors surrounded by bounding boxes bIncludeTrafficSignals boolean false Boolean that takes input whether the traffic sign board/signals are to be considered as actors surrounded by bounding boxes bIncludeRoadsideObjects boolean false Boolean that takes input whether the roadside objects such as traffic cones are to be considered as actors surrounded by bounding boxes bIncludeAll boolean true Boolean that makes sure all actors are to be considered for calculating the bounding boxes Output Attributes Type Description Id int Unique identifier of the detected actor OutBox.Min FVector Coordinates of the top left corner of the bounding box OutBox.Max FVector Coordinates of the bottom left corner of the bounding box Extends.X FVector X coordinate of the bounding box around an actor Extends.Y FVector Y coordinate of the bounding box around an actor Extends.Z FVector Z coordinate of the bounding box around an actor Distance float Sensor distance from the actor","title":"Bounding Box Sensor attributes:"},{"location":"BoundingBoxSensor/#output","text":"The box bounds of the detected actor, [x,y,z] coordinates, class ID, distance and whether the detected objects are commpletely inside or outside of the sensor view.","title":"Output:"},{"location":"BoundingBoxSensor/#3-requirements","text":"Primary requiremnent: Annotator.h . Calculating the bounding boxes is done by the file \"BoundingBoxSensor.cpp\", but for visualization (creating boxes over the detected objects in a RGB image) the following files are required: 1. SceneCaptureCamera.cpp 2. SceneCaptureCamera.h While the BoundingBoxSensor itself calculates the bounding box extents, the visulaization of the boxes is performed by the SceneCaptureSensor that uses CanvasDrawItem method to draw colored lines corresponding to the class of each object using the box extents generated by the BoundingBoxSensor.","title":"3. Requirements"},{"location":"CameraSensor/","text":"Module Name Scene Capture Camera Sensor Version Developer 1. Summary The Scene capture camera sensor is designed to capture scenes and generate RGB images in simulation. Along whith RGB image, we can enable 2D and 3D bounding boxes in a captured RGB image. This sensor can apply different post-processing effects for better realism. 2. Method Initally, a new 2D TextureRenderTarget object is created to render the target texture. The size and format of the pixels for the target texture are initialised according to sensor input, with the texture width and height set to the input imageWidth and imageHeight , respectively. Following the initialization of the target texture,the scene is captured using the SceneCaptureComponent provided by unreal engine. The rate at which scene is captured is determined by 1/scanningFrequency . Once a scene has been captured the RGB color information is extraced by reading the pixels of the target texture. This resulting color data is then send to an image data worker, which convets it into an RGB image in JPG format. If bShow2DBounds = true the camera Sensor draws 2D bounding boxes and when bShow3DBounds = true it draws 3D bounding boxes calculated by bounding box sensor. For realism, different post-processing effects are applied to the captured image. To enable these effects, bEnablePostProcessingEffects should be set to true. The types of post-processing effects applied are... 1. Lense Flare 2. Bloom 3. Chromatic Aberration 4. Depth of Field 5. Auto Exposure 6. Motion Blur These effects are provided by Unreal. To known more about these read Unreal-Postprocessing Effects . The data flow of camera sensor is shown below- Camera attributes Input attribute Type Default Description imageWidth int 640 Image width in pixels imageHeight int 480 Image width in pixels scanningFrequency int 10 Capturing frequency of camera in Hz. bShow2DBounds bool false Whether to show 2D Bounding boxes in captured image. bShow3DBounds bool false Whether to show 3D Bounding boxes in captured image. bShowOccludedObjects bool false Whether to show Bounding boxes for occluded objects. bEnablePostProcessingEffects bool false Enable post-processing effects. targetGamma float 2.2 Target gamma value of the camera. fovAngle float 46.0 Horizontal field of view in degrees. shutterSpeed float 1/80.0 The camera shutter speed in seconds. ISO float 200 The camera sensor sensitivity. fstop float 1.4 Opening of the camera lens. Aperture is 1/fstop with typical lens going down to f/1.2 (larger opening). Larger numbers will reduce the Depth of Field effect. FocalDistance float 1000 Distance at which the depth of field effect should be sharp. Measured in cm (UE units). Post-Processing attributes Input attribute Type Default Description bEnableDepthBlur bool false To enable Depth Blur (Depth of Field) post-process effect. depthBlurBladeCount int 5 The number of blades of the diaphragm within the lens between [4,16]. depthBlurAmount float 1.0 Depth blur km for 50% depthBlurRadius float 0.0 Depth blur radius in pixels at 1920x depthBlurMinFStop float 1.2 the maximum opening of the camera lens to control the curvature of blades of the diaphragm. lensFlareIntensity flaot 0.1 Intensity for the lens flare post-process effect, 0.0 for disabling it. lensFlareThreshold float 0.1 Minimum brightness the lens flare starts having effect. lensFlareBokehSize float 8.0 Size of the lens blur (in percent of the view width) that is done with the Bokeh texture. motionBlurIntensity float 0.45 Strength of motion blur [0,1]. 0.0 for disabling it. motionBlurMaxDistortion float 0.35 Max distortion caused by motion blur. Percentage of screen width. 0.0 for disabling it. motionBlurPerObjectSize float 0.1 The minimum projected screen radius for a primitive to be drawn in the velocity pass for motion blur consideration. The radius is a percentage of screen width. motionBlurTargetFPS int 0 Defines the target frames per second (fps) for motion blur. 0 to make motion blur dependent on the actual measured frame rate. bloomThreshold float -1 Minimum luminance the bloom starts having effect. bloomIntensity float 0.67 Intensity for the bloom post-process effect, 0.0 for disabling it. bEnableChromaticAberration bool 10000.0 To enable Chromatic Aberration post-process effect. ChromaticAberrationIntensity float 0.0 Scaling factor to control color shifting, more noticeable on the screen borders. ChromaticAberrationOffset float 0.0 Normalized distance to the center of the image where the effect takes place. Output attributes Captured RGB image of provided height and width in jpg format. 3. Requirements Unreal","title":"Camera"},{"location":"CameraSensor/#module-name","text":"Scene Capture Camera Sensor","title":"Module Name"},{"location":"CameraSensor/#version","text":"","title":"Version"},{"location":"CameraSensor/#developer","text":"","title":"Developer"},{"location":"CameraSensor/#1-summary","text":"The Scene capture camera sensor is designed to capture scenes and generate RGB images in simulation. Along whith RGB image, we can enable 2D and 3D bounding boxes in a captured RGB image. This sensor can apply different post-processing effects for better realism.","title":"1. Summary"},{"location":"CameraSensor/#2-method","text":"Initally, a new 2D TextureRenderTarget object is created to render the target texture. The size and format of the pixels for the target texture are initialised according to sensor input, with the texture width and height set to the input imageWidth and imageHeight , respectively. Following the initialization of the target texture,the scene is captured using the SceneCaptureComponent provided by unreal engine. The rate at which scene is captured is determined by 1/scanningFrequency . Once a scene has been captured the RGB color information is extraced by reading the pixels of the target texture. This resulting color data is then send to an image data worker, which convets it into an RGB image in JPG format. If bShow2DBounds = true the camera Sensor draws 2D bounding boxes and when bShow3DBounds = true it draws 3D bounding boxes calculated by bounding box sensor. For realism, different post-processing effects are applied to the captured image. To enable these effects, bEnablePostProcessingEffects should be set to true. The types of post-processing effects applied are... 1. Lense Flare 2. Bloom 3. Chromatic Aberration 4. Depth of Field 5. Auto Exposure 6. Motion Blur These effects are provided by Unreal. To known more about these read Unreal-Postprocessing Effects . The data flow of camera sensor is shown below-","title":"2. Method"},{"location":"CameraSensor/#camera-attributes","text":"Input attribute Type Default Description imageWidth int 640 Image width in pixels imageHeight int 480 Image width in pixels scanningFrequency int 10 Capturing frequency of camera in Hz. bShow2DBounds bool false Whether to show 2D Bounding boxes in captured image. bShow3DBounds bool false Whether to show 3D Bounding boxes in captured image. bShowOccludedObjects bool false Whether to show Bounding boxes for occluded objects. bEnablePostProcessingEffects bool false Enable post-processing effects. targetGamma float 2.2 Target gamma value of the camera. fovAngle float 46.0 Horizontal field of view in degrees. shutterSpeed float 1/80.0 The camera shutter speed in seconds. ISO float 200 The camera sensor sensitivity. fstop float 1.4 Opening of the camera lens. Aperture is 1/fstop with typical lens going down to f/1.2 (larger opening). Larger numbers will reduce the Depth of Field effect. FocalDistance float 1000 Distance at which the depth of field effect should be sharp. Measured in cm (UE units).","title":"Camera attributes"},{"location":"CameraSensor/#post-processing-attributes","text":"Input attribute Type Default Description bEnableDepthBlur bool false To enable Depth Blur (Depth of Field) post-process effect. depthBlurBladeCount int 5 The number of blades of the diaphragm within the lens between [4,16]. depthBlurAmount float 1.0 Depth blur km for 50% depthBlurRadius float 0.0 Depth blur radius in pixels at 1920x depthBlurMinFStop float 1.2 the maximum opening of the camera lens to control the curvature of blades of the diaphragm. lensFlareIntensity flaot 0.1 Intensity for the lens flare post-process effect, 0.0 for disabling it. lensFlareThreshold float 0.1 Minimum brightness the lens flare starts having effect. lensFlareBokehSize float 8.0 Size of the lens blur (in percent of the view width) that is done with the Bokeh texture. motionBlurIntensity float 0.45 Strength of motion blur [0,1]. 0.0 for disabling it. motionBlurMaxDistortion float 0.35 Max distortion caused by motion blur. Percentage of screen width. 0.0 for disabling it. motionBlurPerObjectSize float 0.1 The minimum projected screen radius for a primitive to be drawn in the velocity pass for motion blur consideration. The radius is a percentage of screen width. motionBlurTargetFPS int 0 Defines the target frames per second (fps) for motion blur. 0 to make motion blur dependent on the actual measured frame rate. bloomThreshold float -1 Minimum luminance the bloom starts having effect. bloomIntensity float 0.67 Intensity for the bloom post-process effect, 0.0 for disabling it. bEnableChromaticAberration bool 10000.0 To enable Chromatic Aberration post-process effect. ChromaticAberrationIntensity float 0.0 Scaling factor to control color shifting, more noticeable on the screen borders. ChromaticAberrationOffset float 0.0 Normalized distance to the center of the image where the effect takes place.","title":"Post-Processing attributes"},{"location":"CameraSensor/#output-attributes","text":"Captured RGB image of provided height and width in jpg format.","title":"Output attributes"},{"location":"CameraSensor/#3-requirements","text":"Unreal","title":"3. Requirements"},{"location":"DepthCamera/","text":"Module Name Semantic Segmentation Camera Version Developer 1. Summary The Depth camera sensor is designed to capture scenes and generate depth images in simulation. 2. Method Initally, a new 2D TextureRenderTarget object is created to render the target texture. The size and format of the pixels for the target texture are initialised according to sensor input, with the texture width and height set to the input imageWidth and imageHeight , respectively. Following the initialization of the target texture,the scene is captured using the SceneCaptureComponent provided by unreal engine. The rate at which scene is captured is determined by 1/scanningFrequency . Once a scene has been captured a Depth material is applied on target texture. The Depth material calcutes the scene depth value for each pixel and clamps the pixel value between 0-1. This resulting depth color data is then send to an image data worker, which convets it into an segmented image in JPG format. Camera attributes Input attribute Type Default Description imageWidth int 640 Image width in pixels imageHeight int 480 Image width in pixels scanningFrequency int 10 Capturing frequency of camera in Hz. bShow2DBounds bool false Whether to show 2D Bounding boxes in captured image. bShow3DBounds bool false Whether to show 3D Bounding boxes in captured image. bShowOccludedObjects bool false Whether to show Bounding boxes for occluded objects. bEnablePostProcessingEffects bool false Enable post-processing effects. targetGamma float 2.2 Target gamma value of the camera. fovAngle float 46.0 Horizontal field of view in degrees. shutterSpeed float 1/80.0 The camera shutter speed in seconds. ISO float 200 The camera sensor sensitivity. fstop float 1.4 Opening of the camera lens. Aperture is 1/fstop with typical lens going down to f/1.2 (larger opening). Larger numbers will reduce the Depth of Field effect. FocalDistance float 1000 Distance at which the depth of field effect should be sharp. Measured in cm (UE units). Post-Processing attributes Input attribute Type Default Description bEnableDepthBlur bool false To enable Depth Blur (Depth of Field) post-process effect. depthBlurBladeCount int 5 The number of blades of the diaphragm within the lens between [4,16]. depthBlurAmount float 1.0 Depth blur km for 50% depthBlurRadius float 0.0 Depth blur radius in pixels at 1920x depthBlurMinFStop float 1.2 the maximum opening of the camera lens to control the curvature of blades of the diaphragm. lensFlareIntensity flaot 0.1 Intensity for the lens flare post-process effect, 0.0 for disabling it. lensFlareThreshold float 0.1 Minimum brightness the lens flare starts having effect. lensFlareBokehSize float 8.0 Size of the lens blur (in percent of the view width) that is done with the Bokeh texture. motionBlurIntensity float 0.45 Strength of motion blur [0,1]. 0.0 for disabling it. motionBlurMaxDistortion float 0.35 Max distortion caused by motion blur. Percentage of screen width. 0.0 for disabling it. motionBlurPerObjectSize float 0.1 The minimum projected screen radius for a primitive to be drawn in the velocity pass for motion blur consideration. The radius is a percentage of screen width. motionBlurTargetFPS int 0 Defines the target frames per second (fps) for motion blur. 0 to make motion blur dependent on the actual measured frame rate. bloomThreshold float -1 Minimum luminance the bloom starts having effect. bloomIntensity float 0.67 Intensity for the bloom post-process effect, 0.0 for disabling it. bEnableChromaticAberration bool 10000.0 To enable Chromatic Aberration post-process effect. ChromaticAberrationIntensity float 0.0 Scaling factor to control color shifting, more noticeable on the screen borders. ChromaticAberrationOffset float 0.0 Normalized distance to the center of the image where the effect takes place. Output attributes Captured segmented image of provided height and width in jpg format. 3. Requirements Unreal","title":"Depth Camera"},{"location":"DepthCamera/#module-name","text":"Semantic Segmentation Camera","title":"Module Name"},{"location":"DepthCamera/#version","text":"","title":"Version"},{"location":"DepthCamera/#developer","text":"","title":"Developer"},{"location":"DepthCamera/#1-summary","text":"The Depth camera sensor is designed to capture scenes and generate depth images in simulation.","title":"1. Summary"},{"location":"DepthCamera/#2-method","text":"Initally, a new 2D TextureRenderTarget object is created to render the target texture. The size and format of the pixels for the target texture are initialised according to sensor input, with the texture width and height set to the input imageWidth and imageHeight , respectively. Following the initialization of the target texture,the scene is captured using the SceneCaptureComponent provided by unreal engine. The rate at which scene is captured is determined by 1/scanningFrequency . Once a scene has been captured a Depth material is applied on target texture. The Depth material calcutes the scene depth value for each pixel and clamps the pixel value between 0-1. This resulting depth color data is then send to an image data worker, which convets it into an segmented image in JPG format.","title":"2. Method"},{"location":"DepthCamera/#camera-attributes","text":"Input attribute Type Default Description imageWidth int 640 Image width in pixels imageHeight int 480 Image width in pixels scanningFrequency int 10 Capturing frequency of camera in Hz. bShow2DBounds bool false Whether to show 2D Bounding boxes in captured image. bShow3DBounds bool false Whether to show 3D Bounding boxes in captured image. bShowOccludedObjects bool false Whether to show Bounding boxes for occluded objects. bEnablePostProcessingEffects bool false Enable post-processing effects. targetGamma float 2.2 Target gamma value of the camera. fovAngle float 46.0 Horizontal field of view in degrees. shutterSpeed float 1/80.0 The camera shutter speed in seconds. ISO float 200 The camera sensor sensitivity. fstop float 1.4 Opening of the camera lens. Aperture is 1/fstop with typical lens going down to f/1.2 (larger opening). Larger numbers will reduce the Depth of Field effect. FocalDistance float 1000 Distance at which the depth of field effect should be sharp. Measured in cm (UE units).","title":"Camera attributes"},{"location":"DepthCamera/#post-processing-attributes","text":"Input attribute Type Default Description bEnableDepthBlur bool false To enable Depth Blur (Depth of Field) post-process effect. depthBlurBladeCount int 5 The number of blades of the diaphragm within the lens between [4,16]. depthBlurAmount float 1.0 Depth blur km for 50% depthBlurRadius float 0.0 Depth blur radius in pixels at 1920x depthBlurMinFStop float 1.2 the maximum opening of the camera lens to control the curvature of blades of the diaphragm. lensFlareIntensity flaot 0.1 Intensity for the lens flare post-process effect, 0.0 for disabling it. lensFlareThreshold float 0.1 Minimum brightness the lens flare starts having effect. lensFlareBokehSize float 8.0 Size of the lens blur (in percent of the view width) that is done with the Bokeh texture. motionBlurIntensity float 0.45 Strength of motion blur [0,1]. 0.0 for disabling it. motionBlurMaxDistortion float 0.35 Max distortion caused by motion blur. Percentage of screen width. 0.0 for disabling it. motionBlurPerObjectSize float 0.1 The minimum projected screen radius for a primitive to be drawn in the velocity pass for motion blur consideration. The radius is a percentage of screen width. motionBlurTargetFPS int 0 Defines the target frames per second (fps) for motion blur. 0 to make motion blur dependent on the actual measured frame rate. bloomThreshold float -1 Minimum luminance the bloom starts having effect. bloomIntensity float 0.67 Intensity for the bloom post-process effect, 0.0 for disabling it. bEnableChromaticAberration bool 10000.0 To enable Chromatic Aberration post-process effect. ChromaticAberrationIntensity float 0.0 Scaling factor to control color shifting, more noticeable on the screen borders. ChromaticAberrationOffset float 0.0 Normalized distance to the center of the image where the effect takes place.","title":"Post-Processing attributes"},{"location":"DepthCamera/#output-attributes","text":"Captured segmented image of provided height and width in jpg format.","title":"Output attributes"},{"location":"DepthCamera/#3-requirements","text":"Unreal","title":"3. Requirements"},{"location":"LLM-Scenario-Builder/","text":"LLM Scenario Builder Version 1.0 Developer: Meet Maratha NOTE: This is a test repository and there may be some errors present. The user should be well versed in coding and be able to handle them. 1. Task List [x] Check which LLM's can be used for this purpose and how do they fare in comparision to each other. [x] Generate a list of prompts that can be used for LLM's to generate a condensed scenario data. [x] Display a method we can use to generate scenarios from the scenario data provided by LLM's using scenariogeneration module. [x] Create a basic pipeline that can be made more robust in the future to generate more scenarios. [x] Integration with Permutation and Combination module. [ ] To update the README/User guide. [ ] Choose the LLM that we will be using to generate scenarios. [ ] A parser to parse the output from the LLM into a standard format which we can save at a location to be used in future. [ ] A parser to parse the stored standard format of scenario description in a text file and generate a python dictionary from it. [ ] Make the scenario generation methods more robust to handle edge scenarios and not produce errors as much as possible. [ ] Profile the code and try to optimize it. 2. Summary This module is responsible for generation of OpenScenario files with the help of Large Language Models (LLMs) such as ChatGPT and GoogleGemini . In this pipeline we decide the type of scenario to be generated and then use LLMs API prompting to get relevant information about the scenario and the environment. We then use our method to generate the road network, which is stored as OpenDrive file and scenario description that is stored as an OpenScenario 1.2.0. To get information about the scenario and environment through LLM we use our proprietary solution that ensures reliable and accurate files. This proprietary solution consits of data generation, validation, and error free output file generation. This module uses OpenScenario 1.2.0 as its output format for the scenario. Before saving the scenario in database we perform a syntax check on it to ensure that there is no error in the generate OpenScenario file. We can use this generated OpenScenario file defining the maneuver in combination with our Variation Engine which can perform Permutation and Combination to generate variations of this base scenario to provide you bulk of OpenScenario files for one/collection of Base scenario(s). 3. Method 3.1 Inputs Scenario Type: The type of scenario that has to be generated. Number of Scenarios: The number of scenarios that we need as output. 3.2 LLM Data Extractor This is our proprietary module that uses the inputs to generate a compressed, reliable, and valid datastructure that we can use to generate OpenDrive and OpenScenario files. 3.3 Road Network Information This is a datastructure that stores information about the road network which we can use to generate OpenDrive file. 3.4 Scenario Information This stores information about the vehicles, objects, and their interaction in a datastructure that can be used to generate OpenScenario files. 3.5 Road Network Generator This module takes the road network information generated using LLM Data Extractor and prepares a cohesive OpenDrive file with all the roads, their connections, and roadside objects. 3.6 Scenario Generator This takes the scenario information generated by LLM Data Extractor and generates an OpenScenario file that has all the vehicles, objects, pedestrians, and thier interactions. 3.7 Syntax Checker This takes input as OpenDrive or OpenScenario file and checks for any syntax error. If it finds a syntax error in one of these files, it drops both of them and moves on to other processes. 3.8 Output OpenDrive File: This is the final OpenDrive file which has been generated using LLM Scenario Builder. OpenScenario File: This is the final OpenScenario file which has been generated using LLM Scenario Builder. Both of these files are stored in database for further computation on them. 4. Requirement Installed Python version 3.10 ScenarioGeneration Python package. LLM API key 4. How-to-guide Install python version 3.10 Open a command prompt (cmd) or terminal on your system. For Windows: - Open the folder in file explorer where you want to clone the repository - Click on the white space after the path which is displayed in file explorer. Remove the path and type cmd instead of it, and click enter. This will open the command prompt window. For Linux: - Open the terminal as you would normally do in your system. Clone the github repository using the following command git clone https://github.com/MeetMaratha/ScenarioGeneration-Test.git Change direcotries to the cloned repository using following commands: cd ScenarioGeneration-Test Make a virtual environment to work. This is only needed to separate the project from the main system install. You can use the following command in the cloned github folder to generate the virtual environment For Windows and Linux distributions other than Ubuntu/Debian: python -m venv sg_venv For Ubuntu/Debian use following command: python3 -m venv sg_venv Activate the virtual environment using the following command in the cloned github folder For Bash/ZSH shell: source sg_ven/bin/activate For Windows Command Prompt/Powershell/Windows Terminal: sg_venv\\Scripts\\activate For Fish shell: source sg_venv/bin/activate.fish Install the python packages required using the following command pip install -r requirements.txt 5. Tutorial Use the following prompt to generate 20 scenarios of any scenario type using one of the suggested LLM's. We used a place holder [Scenario Name] here, remember to replace it with valid value. Act as a scenario writer. Generate 20 different [Scenario Name] scenario that I can use as a base and use permutations and combinations to generate multiple scenarios from it. Format the output as a list of paragraphs. Now choose one of the scenarios from the output and generate a xodr data dictionary for it using the LLM and the following prompt. Place the scenario description you have choosen where there is a place holder [Scenario Description] profile picture Generate a structured dictionary data for the following scenario. The comments in \"()\" are somethings that I need you to pay extra attention to: \" [Scenario Description] \" Fill the values for each of these tags (need to be numerical). Following tags should be present compulsorily and they should be presented in the following structure: 1. Number of roads (calculate it properly) 2. Number of lanes in each road (calculate it properly) 3. Number of junctions (Do not put unnecessary junctions in the network, if there are more than 3 roads then only a junction can be formed, so check whether that is the case or not based on the description) 4. Road Information a. Lane Information i. Length of lane ii. Direction of driving (calculate it properly, based on the description, +1 or -1 representing left and right driving respectively) iii. Width of lane iv. Id of lane b. Predecessor and Successor of roads (calculate it properly, if there are any. Neighboring roads are not predecessor and successors. Also notify the first road in the road network, which would not have any predecessor with -1) c. Name of the road d. Angle of the road (If there is a junction) If you could, provide an image that I can use to understand how road network should look like. Format the output as python dictionary. Now try to generate a XOSC data description using the LLM for the chosen scenario using the following prompt. Similar to the previous step copy the scenario description where there is a place holder [Scenario Description] Generate a structured dictionary data for the following scenario. The comments in \"()\" are somethings that I need you to pay extra attention to: \" [Scenario Description] \" Fill the values for each of these tags (need to be numerical). Following tags should be present compulsorily and they should be presented in the following structure: 1. Number of Entities (Based on previoius response) 2. Information of entities (Based on previous response for each entity) a. Name of Entity b. Dimensions of Entity (Length, Width, Height) c. Displacement in X along a lane of the road d. Action associated to this entity i. Name of the action ii. Trigger for the action (Keep this as time) iii. When does the action occur? iv. How long does the action occur? v. If there is a lane/road change due to this action, specify it e. Initial speed of the vehicle 3. Weather Format the output as python dictionary Now make a copy of any of the files in the Input folder and replace/add the values as necessary. This is currently a manual task but would automated in future. You can use the following command to make the copy (We suggest you use this command and while using it be in the root directory of the repository) For Linux systems: cp Inputs/Test_CI_1.py Inputs/New_File.py For Windows system: copy \"Inputs\\Test_CI_1.py\" \"Inputs\\New_File.py\" This commands will generate a python file that you will need to update in the Inputs folder and the file would be called New_File.py Rename the file according to the type of the scenario description. This is the only labeling of file that will help you seperate it from others, so do it correctly. We recommend the following naming: Test_{Type of Scenario in Abbreviation}_{Next Number of file}.py We recommend the following abbreviations Type Abbreviation Lane Change LC Left Turn LT Right Turn RT Cut-in CI Cut-out CO Left turn/Right Turn LT_RT Now manually edit the python script saved and add the information to the custom_parameters python dictionary. Add the things to the relevant places as mentioned using the comments. Once you are satisfied with your editing, run the main.py file using the following commands: For Windows and Linux distributions other than Ubuntu/Debian: python main.py For Ubuntu/Debian use following command: python3 main.py This will launch an interactive shell where you can choose the file for which you want to generate scenario and realizations. Once it is done, the outputs will be stored in the Outputs folder. Following subfolders will have following outputs: XODR: The XODR files generated for your choice. XOSC: The XOSC files generated for your choice. Realizations: The permutation and combination of your generated XOSC file will be stored in it inside a folder for each scenario. The folder name is based on the name of the input python file.","title":"LLM Scenario Builder"},{"location":"LLM-Scenario-Builder/#llm-scenario-builder","text":"","title":"LLM Scenario Builder"},{"location":"LLM-Scenario-Builder/#version-10","text":"","title":"Version 1.0"},{"location":"LLM-Scenario-Builder/#developer-meet-maratha","text":"NOTE: This is a test repository and there may be some errors present. The user should be well versed in coding and be able to handle them.","title":"Developer: Meet Maratha"},{"location":"LLM-Scenario-Builder/#1-task-list","text":"[x] Check which LLM's can be used for this purpose and how do they fare in comparision to each other. [x] Generate a list of prompts that can be used for LLM's to generate a condensed scenario data. [x] Display a method we can use to generate scenarios from the scenario data provided by LLM's using scenariogeneration module. [x] Create a basic pipeline that can be made more robust in the future to generate more scenarios. [x] Integration with Permutation and Combination module. [ ] To update the README/User guide. [ ] Choose the LLM that we will be using to generate scenarios. [ ] A parser to parse the output from the LLM into a standard format which we can save at a location to be used in future. [ ] A parser to parse the stored standard format of scenario description in a text file and generate a python dictionary from it. [ ] Make the scenario generation methods more robust to handle edge scenarios and not produce errors as much as possible. [ ] Profile the code and try to optimize it.","title":"1. Task List"},{"location":"LLM-Scenario-Builder/#2-summary","text":"This module is responsible for generation of OpenScenario files with the help of Large Language Models (LLMs) such as ChatGPT and GoogleGemini . In this pipeline we decide the type of scenario to be generated and then use LLMs API prompting to get relevant information about the scenario and the environment. We then use our method to generate the road network, which is stored as OpenDrive file and scenario description that is stored as an OpenScenario 1.2.0. To get information about the scenario and environment through LLM we use our proprietary solution that ensures reliable and accurate files. This proprietary solution consits of data generation, validation, and error free output file generation. This module uses OpenScenario 1.2.0 as its output format for the scenario. Before saving the scenario in database we perform a syntax check on it to ensure that there is no error in the generate OpenScenario file. We can use this generated OpenScenario file defining the maneuver in combination with our Variation Engine which can perform Permutation and Combination to generate variations of this base scenario to provide you bulk of OpenScenario files for one/collection of Base scenario(s).","title":"2. Summary"},{"location":"LLM-Scenario-Builder/#3-method","text":"","title":"3. Method"},{"location":"LLM-Scenario-Builder/#31-inputs","text":"Scenario Type: The type of scenario that has to be generated. Number of Scenarios: The number of scenarios that we need as output.","title":"3.1 Inputs"},{"location":"LLM-Scenario-Builder/#32-llm-data-extractor","text":"This is our proprietary module that uses the inputs to generate a compressed, reliable, and valid datastructure that we can use to generate OpenDrive and OpenScenario files.","title":"3.2 LLM Data Extractor"},{"location":"LLM-Scenario-Builder/#33-road-network-information","text":"This is a datastructure that stores information about the road network which we can use to generate OpenDrive file.","title":"3.3 Road Network Information"},{"location":"LLM-Scenario-Builder/#34-scenario-information","text":"This stores information about the vehicles, objects, and their interaction in a datastructure that can be used to generate OpenScenario files.","title":"3.4 Scenario Information"},{"location":"LLM-Scenario-Builder/#35-road-network-generator","text":"This module takes the road network information generated using LLM Data Extractor and prepares a cohesive OpenDrive file with all the roads, their connections, and roadside objects.","title":"3.5 Road Network Generator"},{"location":"LLM-Scenario-Builder/#36-scenario-generator","text":"This takes the scenario information generated by LLM Data Extractor and generates an OpenScenario file that has all the vehicles, objects, pedestrians, and thier interactions.","title":"3.6 Scenario Generator"},{"location":"LLM-Scenario-Builder/#37-syntax-checker","text":"This takes input as OpenDrive or OpenScenario file and checks for any syntax error. If it finds a syntax error in one of these files, it drops both of them and moves on to other processes.","title":"3.7 Syntax Checker"},{"location":"LLM-Scenario-Builder/#38-output","text":"OpenDrive File: This is the final OpenDrive file which has been generated using LLM Scenario Builder. OpenScenario File: This is the final OpenScenario file which has been generated using LLM Scenario Builder. Both of these files are stored in database for further computation on them.","title":"3.8 Output"},{"location":"LLM-Scenario-Builder/#4-requirement","text":"Installed Python version 3.10 ScenarioGeneration Python package. LLM API key","title":"4. Requirement"},{"location":"LLM-Scenario-Builder/#4-how-to-guide","text":"Install python version 3.10 Open a command prompt (cmd) or terminal on your system. For Windows: - Open the folder in file explorer where you want to clone the repository - Click on the white space after the path which is displayed in file explorer. Remove the path and type cmd instead of it, and click enter. This will open the command prompt window. For Linux: - Open the terminal as you would normally do in your system. Clone the github repository using the following command git clone https://github.com/MeetMaratha/ScenarioGeneration-Test.git Change direcotries to the cloned repository using following commands: cd ScenarioGeneration-Test Make a virtual environment to work. This is only needed to separate the project from the main system install. You can use the following command in the cloned github folder to generate the virtual environment For Windows and Linux distributions other than Ubuntu/Debian: python -m venv sg_venv For Ubuntu/Debian use following command: python3 -m venv sg_venv Activate the virtual environment using the following command in the cloned github folder For Bash/ZSH shell: source sg_ven/bin/activate For Windows Command Prompt/Powershell/Windows Terminal: sg_venv\\Scripts\\activate For Fish shell: source sg_venv/bin/activate.fish Install the python packages required using the following command pip install -r requirements.txt","title":"4. How-to-guide"},{"location":"LLM-Scenario-Builder/#5-tutorial","text":"Use the following prompt to generate 20 scenarios of any scenario type using one of the suggested LLM's. We used a place holder [Scenario Name] here, remember to replace it with valid value. Act as a scenario writer. Generate 20 different [Scenario Name] scenario that I can use as a base and use permutations and combinations to generate multiple scenarios from it. Format the output as a list of paragraphs. Now choose one of the scenarios from the output and generate a xodr data dictionary for it using the LLM and the following prompt. Place the scenario description you have choosen where there is a place holder [Scenario Description] profile picture Generate a structured dictionary data for the following scenario. The comments in \"()\" are somethings that I need you to pay extra attention to: \" [Scenario Description] \" Fill the values for each of these tags (need to be numerical). Following tags should be present compulsorily and they should be presented in the following structure: 1. Number of roads (calculate it properly) 2. Number of lanes in each road (calculate it properly) 3. Number of junctions (Do not put unnecessary junctions in the network, if there are more than 3 roads then only a junction can be formed, so check whether that is the case or not based on the description) 4. Road Information a. Lane Information i. Length of lane ii. Direction of driving (calculate it properly, based on the description, +1 or -1 representing left and right driving respectively) iii. Width of lane iv. Id of lane b. Predecessor and Successor of roads (calculate it properly, if there are any. Neighboring roads are not predecessor and successors. Also notify the first road in the road network, which would not have any predecessor with -1) c. Name of the road d. Angle of the road (If there is a junction) If you could, provide an image that I can use to understand how road network should look like. Format the output as python dictionary. Now try to generate a XOSC data description using the LLM for the chosen scenario using the following prompt. Similar to the previous step copy the scenario description where there is a place holder [Scenario Description] Generate a structured dictionary data for the following scenario. The comments in \"()\" are somethings that I need you to pay extra attention to: \" [Scenario Description] \" Fill the values for each of these tags (need to be numerical). Following tags should be present compulsorily and they should be presented in the following structure: 1. Number of Entities (Based on previoius response) 2. Information of entities (Based on previous response for each entity) a. Name of Entity b. Dimensions of Entity (Length, Width, Height) c. Displacement in X along a lane of the road d. Action associated to this entity i. Name of the action ii. Trigger for the action (Keep this as time) iii. When does the action occur? iv. How long does the action occur? v. If there is a lane/road change due to this action, specify it e. Initial speed of the vehicle 3. Weather Format the output as python dictionary Now make a copy of any of the files in the Input folder and replace/add the values as necessary. This is currently a manual task but would automated in future. You can use the following command to make the copy (We suggest you use this command and while using it be in the root directory of the repository) For Linux systems: cp Inputs/Test_CI_1.py Inputs/New_File.py For Windows system: copy \"Inputs\\Test_CI_1.py\" \"Inputs\\New_File.py\" This commands will generate a python file that you will need to update in the Inputs folder and the file would be called New_File.py Rename the file according to the type of the scenario description. This is the only labeling of file that will help you seperate it from others, so do it correctly. We recommend the following naming: Test_{Type of Scenario in Abbreviation}_{Next Number of file}.py We recommend the following abbreviations Type Abbreviation Lane Change LC Left Turn LT Right Turn RT Cut-in CI Cut-out CO Left turn/Right Turn LT_RT Now manually edit the python script saved and add the information to the custom_parameters python dictionary. Add the things to the relevant places as mentioned using the comments. Once you are satisfied with your editing, run the main.py file using the following commands: For Windows and Linux distributions other than Ubuntu/Debian: python main.py For Ubuntu/Debian use following command: python3 main.py This will launch an interactive shell where you can choose the file for which you want to generate scenario and realizations. Once it is done, the outputs will be stored in the Outputs folder. Following subfolders will have following outputs: XODR: The XODR files generated for your choice. XOSC: The XOSC files generated for your choice. Realizations: The permutation and combination of your generated XOSC file will be stored in it inside a folder for each scenario. The folder name is based on the name of the input python file.","title":"5. Tutorial"},{"location":"LidarSensor/","text":"Module Name Rotating LIDAR Sensor Version Developer 1. Summary The Lidar sensor is designed to simulate a rotating LIDAR sensor using ray-casting . It utilizes Lidar parameters such as pluse repetation frequency, scanning frequency, field of view, and range to generate a point cloud output. The resulting point cloud contains location, time, distance, intensity, label and scan angle information in ply format. The sensor design incorporates multithreading for optimization. 2. Method To simulate the rotating Lidar sensor, the sensor is first initiallised by computing vertical angle for each channel/laser within the Vertical FOV. After that, the no. of fires per scan and per second is calculatd using FiresPerScan = SensorParameters.Prf / SensorParameters.ScanningFrequency / SensorParameters.Channels and FiresPerSec = FiresPerScan * SensorParameters.ScanningFrequency . To ensure that the fires per second remain consistant and independent of simulation's FPS they are multiplited by 1/FPS resulting in FiresPerSec = FiresPerScan * SensorParameters.ScanningFrequency * (1/FPS) . Ray-casting is then performed in parallel for each channel/laser to increase efficiency and decrease computation time. \\ The point cloud obtained from ray-casting contains information about the hit point's location (in xyz), time , distance , intensity , label and scan angle . The intensity of hit point is calculate using the formulae: $ I_{\\text{return}} = I_{\\text{emit}} \\times R_{\\text{rel}} \\times \\cos \\theta \\times \\exp(-\\sigma_{\\text{air}} \\times D) $ Where $ I_{\\text{return}}$ is the intensity of the retuened laser pulse, $ I_{\\text{emit}} $ is the intensity of the emitted laser pulse, $ R_{\\text{rel}} $ denotes the reflectivity of the surface material, $ \\cos \\theta $ is the reflection rate w.r.t angle of incidence $ \\theta $ and $ \\exp(-\\sigma_{\\text{air}} \\times D) $ is the atmosphere attenuation rate, $ \\sigma_{\\text{air}} $ is atmosphere attenuation rate cofficient and $ D $ is distance of hit point form Lidar. The reflectance of material is stored in reflectance.txt file at four different wavelengths refered from Spectral Libraray provided by U.S Geological Survey. The reflectance file looks like this- The reflectance file contains three columns: the first column lists the material name, the second column denotes the wavelength (in micrometers), and the third and final column displays the reflectance percentage corresponding to each wavelength. The label of hit point is its semantic tag mapped with the class. The class-tag mapping with semantic color is shown below- | Tag | Class | Color | |----|--------------------|----------------------| | 1 | BoundaryFencing | FColor{112, 160, 160}| | 2 | Buildings | FColor{116, 116, 116}| | 3 | BusStand | FColor{207, 207, 111}| | 4 | Decals | FColor{178, 139, 66} | | 5 | Fences | FColor{153, 153, 187}| | 6 | Pedestrians | FColor{228, 79, 109} | | 7 | Props | FColor{234, 175, 2} | | 8 | RailTrack | FColor{230, 150, 140}| | 9 | RoadLaneMarkings | FColor{196, 199, 198}| | 10 | RoadMeshes | FColor{160, 112, 160}| | 11 | RoadsideFurniture | FColor{220, 151, 245}| | 12 | SideWalk | FColor{246, 90, 236} | | 13 | StreetLights | FColor{178, 178, 178}| | 14 | Terrain | FColor{178, 252, 177}| | 15 | TrafficSigns | FColor{229, 227, 64} | | 16 | TransmissionTower | FColor{178, 178, 178}| | 17 | Vegetation | FColor{145, 170, 89} | | 18 | Vehicles | FColor{63, 64, 170} | | 19 | TrafficLights | FColor{181, 235, 231}| | 20 | SmallPole | FColor{178, 178, 178}| | 21 | Sky | FColor{116, 162, 198}| | 22 | Rider | FColor{254, 64, 66} | | 23 | Truck | FColor{64, 64, 116} | | 24 | Bus | FColor{64, 109, 140} | | 25 | Train | FColor{65, 124, 140} | | 26 | Motorcycle | FColor{64, 64, 236} | | 27 | Bicycle | FColor{153, 72, 87} | | 28 | Bridge | FColor{175, 139, 139}| | 29 | Tunnel | FColor{178, 153, 131}| To obtain better realism random noise with stdDev=0.005 has been added to the hit points.\\ The resulted point cloud is passed to class LidarDataWorker to write the output in a .ply file. The writing process of data happens parallel to ray-casting in background. The data flow of Lidar sensor is shown below- Lidar attributes Input attribute Type Default Description prf int 500000 Pulse Repetition Frequency: No. of points generated by all lasers per second. channels int 64 Number of lasers. scanningFrequency int 1 Liadr scanning frequency in Hz. maxRange float 10000.0 Maximum distance at which a point can be detected by the Lidar sensor (in cm). minRange float 0.0 Minimum distance at which a point can be detected by the Lidar sensor (in cm). negVerticalFOV float -25.0 Angle in degrees of the lowest laser. posVerticalFOV float 15.0 Angle in degrees of the highest laser. atmosphereAttenuationRate float 0.004 Coefficient that measures the LIDAR instensity loss per meter, wavelength float 1.4 Wavelength at which Lidar sensor operates (in micrometer). Output attributes Sensor data attribute Type Description x float X coordinate of hit point. y float Y coordinate of hit point. z float Z coordinate of hit point. intenstity float Intensity of the reflected leaser from the hit point. time float Simulation time of the measurement in seconds since the beginning of the episode. label uint Label of hit point (see class-label mapping). r uint red value of the color of label. g uint green value of the color of label. b uint blue value of the color of label. scan_angle float Horizontal angle of the hit point. distance float Distance of hit point from Lidar sensor (im cm). 3. Requirements Unreal Engine","title":"Rotation Lidar"},{"location":"LidarSensor/#module-name","text":"Rotating LIDAR Sensor","title":"Module Name"},{"location":"LidarSensor/#version","text":"","title":"Version"},{"location":"LidarSensor/#developer","text":"","title":"Developer"},{"location":"LidarSensor/#1-summary","text":"The Lidar sensor is designed to simulate a rotating LIDAR sensor using ray-casting . It utilizes Lidar parameters such as pluse repetation frequency, scanning frequency, field of view, and range to generate a point cloud output. The resulting point cloud contains location, time, distance, intensity, label and scan angle information in ply format. The sensor design incorporates multithreading for optimization.","title":"1. Summary"},{"location":"LidarSensor/#2-method","text":"To simulate the rotating Lidar sensor, the sensor is first initiallised by computing vertical angle for each channel/laser within the Vertical FOV. After that, the no. of fires per scan and per second is calculatd using FiresPerScan = SensorParameters.Prf / SensorParameters.ScanningFrequency / SensorParameters.Channels and FiresPerSec = FiresPerScan * SensorParameters.ScanningFrequency . To ensure that the fires per second remain consistant and independent of simulation's FPS they are multiplited by 1/FPS resulting in FiresPerSec = FiresPerScan * SensorParameters.ScanningFrequency * (1/FPS) . Ray-casting is then performed in parallel for each channel/laser to increase efficiency and decrease computation time. \\ The point cloud obtained from ray-casting contains information about the hit point's location (in xyz), time , distance , intensity , label and scan angle . The intensity of hit point is calculate using the formulae: $ I_{\\text{return}} = I_{\\text{emit}} \\times R_{\\text{rel}} \\times \\cos \\theta \\times \\exp(-\\sigma_{\\text{air}} \\times D) $ Where $ I_{\\text{return}}$ is the intensity of the retuened laser pulse, $ I_{\\text{emit}} $ is the intensity of the emitted laser pulse, $ R_{\\text{rel}} $ denotes the reflectivity of the surface material, $ \\cos \\theta $ is the reflection rate w.r.t angle of incidence $ \\theta $ and $ \\exp(-\\sigma_{\\text{air}} \\times D) $ is the atmosphere attenuation rate, $ \\sigma_{\\text{air}} $ is atmosphere attenuation rate cofficient and $ D $ is distance of hit point form Lidar. The reflectance of material is stored in reflectance.txt file at four different wavelengths refered from Spectral Libraray provided by U.S Geological Survey. The reflectance file looks like this- The reflectance file contains three columns: the first column lists the material name, the second column denotes the wavelength (in micrometers), and the third and final column displays the reflectance percentage corresponding to each wavelength. The label of hit point is its semantic tag mapped with the class. The class-tag mapping with semantic color is shown below- | Tag | Class | Color | |----|--------------------|----------------------| | 1 | BoundaryFencing | FColor{112, 160, 160}| | 2 | Buildings | FColor{116, 116, 116}| | 3 | BusStand | FColor{207, 207, 111}| | 4 | Decals | FColor{178, 139, 66} | | 5 | Fences | FColor{153, 153, 187}| | 6 | Pedestrians | FColor{228, 79, 109} | | 7 | Props | FColor{234, 175, 2} | | 8 | RailTrack | FColor{230, 150, 140}| | 9 | RoadLaneMarkings | FColor{196, 199, 198}| | 10 | RoadMeshes | FColor{160, 112, 160}| | 11 | RoadsideFurniture | FColor{220, 151, 245}| | 12 | SideWalk | FColor{246, 90, 236} | | 13 | StreetLights | FColor{178, 178, 178}| | 14 | Terrain | FColor{178, 252, 177}| | 15 | TrafficSigns | FColor{229, 227, 64} | | 16 | TransmissionTower | FColor{178, 178, 178}| | 17 | Vegetation | FColor{145, 170, 89} | | 18 | Vehicles | FColor{63, 64, 170} | | 19 | TrafficLights | FColor{181, 235, 231}| | 20 | SmallPole | FColor{178, 178, 178}| | 21 | Sky | FColor{116, 162, 198}| | 22 | Rider | FColor{254, 64, 66} | | 23 | Truck | FColor{64, 64, 116} | | 24 | Bus | FColor{64, 109, 140} | | 25 | Train | FColor{65, 124, 140} | | 26 | Motorcycle | FColor{64, 64, 236} | | 27 | Bicycle | FColor{153, 72, 87} | | 28 | Bridge | FColor{175, 139, 139}| | 29 | Tunnel | FColor{178, 153, 131}| To obtain better realism random noise with stdDev=0.005 has been added to the hit points.\\ The resulted point cloud is passed to class LidarDataWorker to write the output in a .ply file. The writing process of data happens parallel to ray-casting in background. The data flow of Lidar sensor is shown below-","title":"2. Method"},{"location":"LidarSensor/#lidar-attributes","text":"Input attribute Type Default Description prf int 500000 Pulse Repetition Frequency: No. of points generated by all lasers per second. channels int 64 Number of lasers. scanningFrequency int 1 Liadr scanning frequency in Hz. maxRange float 10000.0 Maximum distance at which a point can be detected by the Lidar sensor (in cm). minRange float 0.0 Minimum distance at which a point can be detected by the Lidar sensor (in cm). negVerticalFOV float -25.0 Angle in degrees of the lowest laser. posVerticalFOV float 15.0 Angle in degrees of the highest laser. atmosphereAttenuationRate float 0.004 Coefficient that measures the LIDAR instensity loss per meter, wavelength float 1.4 Wavelength at which Lidar sensor operates (in micrometer).","title":"Lidar attributes"},{"location":"LidarSensor/#output-attributes","text":"Sensor data attribute Type Description x float X coordinate of hit point. y float Y coordinate of hit point. z float Z coordinate of hit point. intenstity float Intensity of the reflected leaser from the hit point. time float Simulation time of the measurement in seconds since the beginning of the episode. label uint Label of hit point (see class-label mapping). r uint red value of the color of label. g uint green value of the color of label. b uint blue value of the color of label. scan_angle float Horizontal angle of the hit point. distance float Distance of hit point from Lidar sensor (im cm).","title":"Output attributes"},{"location":"LidarSensor/#3-requirements","text":"Unreal Engine","title":"3. Requirements"},{"location":"LightsModelling/","text":"Traffic Lights Version 1.0 Modeler - Manasvi Kale 1. Summary This document helps you create a traffic lights blueprint which is in alignment with traffic lights controller, during the Simulation vehicles and pedestrians will only follow the traffic lights created by these specifications in our simulator. 2. Method A blueprint deriving from traffic lights base class is created which inherits light controlling properties and functions of the base class. This Blueprint is then further given to controller for realistic working of traffic light at a junction in environment. Refer How-to-guide to set up other independent compoents required for working of traffic lights. flowchart - 3. Requirements Static Mesh of Traffic Light Relevant materials for light mesh Trafficlights base class Traffic Lights Group Controller Crosswalk blueprints 5.1. 3D asset of crosswalk 5.2. Material Blueprints 5.3. Crosswalk base class 4. How-to-guide 4.1 Step 1 - Creating Traffic Light Blueprint Step 1 - Create / Import static mesh for traffic lights. Step 2 - Create a Blueprint in the folder content>blueprints>lights. Step 3 - Inherit a new blueprint from the \u201ctrafficlightbase\u201d class. Step 4 - Add static mesh in the new BP. Step 5 - Set lights and default intensity to 0. Step 6 - Apply and save BP 4.2 Step 1 - Creating Crosswalks Blueprint Step 1 - Create/Import a static mesh for crosswalks. Step 2 - Create a new BP for crosswalks and inherit it from the CrossingBase class. Step 3 - Import static mesh for crosswalks in BP and place it inside the box. Step 4 - Set collision presets for the box as \u201cno collision\u201d Step 5 - Set collision presets for all crosswalks static meshes used as \u201cno collision\u201d Step 6 - Apply and save 4.3 Step 3 - Traffic Lights Group Controller Step 1 - Place traffic lights group controller anywhere in the map. Step 2 - Set phase time and clearance time for the lights. Step 3 - Add the number array elements as no. of lights present in the map. Step 4 - Add the crosswalk array elements as no. of crosswalks in the map. Step 5 - Select the BP of traffic lights in the order you want them to run. Step 6 - For each traffic light add the respective crosswalk in the crosswalk array elements. Ensure the order of traffic lights and the order of their crosswalk should match. Step 7 - Save the map and perform trail runs to ensure correct working of lights and crosswalks. Street Lights Version 1 1. Summary This document aims to create a dynamic Street lights blueprints which will turn on only when the simulation is happening in evening or in night mode and turns off autonatically otherwise. 2. Method A blueprint deriving from street lights base class is created which inherits light controlling properties and functions of the base class. The street lights blueprints are of two types. Single Street Light Double Street Light flowchart - 3. Requirements Street Light 3D model Material blueprints for street light lights base classes 3.1 single street light base class 3.2 double street light base class 4. How-to-guide Step 1 - Create / Import static mesh for steet lights. Step 2 - Create a Blueprint in the folder content>blueprints>lights. Step 3 - Inherit a new blueprint from the \u201csinglestreetlightbase\u201d class. Step 4 - Add static mesh in the new BP. Step 5 - Set lights and default intensity to 0. Step 6 - Apply and save BP The procedure to create street light blueprint from double street light base class is same. The base class changes to \"DoubleStrteetLights\" base class. 5. Tutorial","title":"Lights"},{"location":"LightsModelling/#traffic-lights","text":"","title":"Traffic Lights"},{"location":"LightsModelling/#version-10","text":"","title":"Version 1.0"},{"location":"LightsModelling/#modeler-manasvi-kale","text":"","title":"Modeler - Manasvi Kale"},{"location":"LightsModelling/#1-summary","text":"This document helps you create a traffic lights blueprint which is in alignment with traffic lights controller, during the Simulation vehicles and pedestrians will only follow the traffic lights created by these specifications in our simulator.","title":"1. Summary"},{"location":"LightsModelling/#2-method","text":"A blueprint deriving from traffic lights base class is created which inherits light controlling properties and functions of the base class. This Blueprint is then further given to controller for realistic working of traffic light at a junction in environment. Refer How-to-guide to set up other independent compoents required for working of traffic lights. flowchart -","title":"2. Method"},{"location":"LightsModelling/#3-requirements","text":"Static Mesh of Traffic Light Relevant materials for light mesh Trafficlights base class Traffic Lights Group Controller Crosswalk blueprints 5.1. 3D asset of crosswalk 5.2. Material Blueprints 5.3. Crosswalk base class","title":"3. Requirements"},{"location":"LightsModelling/#4-how-to-guide","text":"","title":"4. How-to-guide"},{"location":"LightsModelling/#41-step-1-creating-traffic-light-blueprint","text":"Step 1 - Create / Import static mesh for traffic lights. Step 2 - Create a Blueprint in the folder content>blueprints>lights. Step 3 - Inherit a new blueprint from the \u201ctrafficlightbase\u201d class. Step 4 - Add static mesh in the new BP. Step 5 - Set lights and default intensity to 0. Step 6 - Apply and save BP","title":"4.1 Step 1 - Creating Traffic Light Blueprint"},{"location":"LightsModelling/#42-step-1-creating-crosswalks-blueprint","text":"Step 1 - Create/Import a static mesh for crosswalks. Step 2 - Create a new BP for crosswalks and inherit it from the CrossingBase class. Step 3 - Import static mesh for crosswalks in BP and place it inside the box. Step 4 - Set collision presets for the box as \u201cno collision\u201d Step 5 - Set collision presets for all crosswalks static meshes used as \u201cno collision\u201d Step 6 - Apply and save","title":"4.2 Step 1 -  Creating Crosswalks Blueprint"},{"location":"LightsModelling/#43-step-3-traffic-lights-group-controller","text":"Step 1 - Place traffic lights group controller anywhere in the map. Step 2 - Set phase time and clearance time for the lights. Step 3 - Add the number array elements as no. of lights present in the map. Step 4 - Add the crosswalk array elements as no. of crosswalks in the map. Step 5 - Select the BP of traffic lights in the order you want them to run. Step 6 - For each traffic light add the respective crosswalk in the crosswalk array elements. Ensure the order of traffic lights and the order of their crosswalk should match. Step 7 - Save the map and perform trail runs to ensure correct working of lights and crosswalks.","title":"4.3 Step 3 -  Traffic Lights Group Controller"},{"location":"LightsModelling/#street-lights","text":"","title":"Street Lights"},{"location":"LightsModelling/#version-1","text":"","title":"Version 1"},{"location":"LightsModelling/#1-summary_1","text":"This document aims to create a dynamic Street lights blueprints which will turn on only when the simulation is happening in evening or in night mode and turns off autonatically otherwise.","title":"1. Summary"},{"location":"LightsModelling/#2-method_1","text":"A blueprint deriving from street lights base class is created which inherits light controlling properties and functions of the base class. The street lights blueprints are of two types. Single Street Light Double Street Light flowchart -","title":"2. Method"},{"location":"LightsModelling/#3-requirements_1","text":"Street Light 3D model Material blueprints for street light lights base classes 3.1 single street light base class 3.2 double street light base class","title":"3. Requirements"},{"location":"LightsModelling/#4-how-to-guide_1","text":"","title":"4. How-to-guide"},{"location":"LightsModelling/#step-1-create-import-static-mesh-for-steet-lights","text":"","title":"Step 1 - Create / Import static mesh for steet lights."},{"location":"LightsModelling/#step-2-create-a-blueprint-in-the-folder-contentblueprintslights","text":"","title":"Step 2 - Create a Blueprint in the folder content&gt;blueprints&gt;lights."},{"location":"LightsModelling/#step-3-inherit-a-new-blueprint-from-the-singlestreetlightbase-class","text":"","title":"Step 3 - Inherit a new blueprint from the \u201csinglestreetlightbase\u201d class."},{"location":"LightsModelling/#step-4-add-static-mesh-in-the-new-bp","text":"","title":"Step 4 - Add static mesh in the new BP."},{"location":"LightsModelling/#step-5-set-lights-and-default-intensity-to-0","text":"","title":"Step 5 - Set lights and default intensity to 0."},{"location":"LightsModelling/#step-6-apply-and-save-bp","text":"The procedure to create street light blueprint from double street light base class is same. The base class changes to \"DoubleStrteetLights\" base class.","title":"Step 6 - Apply and save BP"},{"location":"LightsModelling/#5-tutorial","text":"","title":"5. Tutorial"},{"location":"ManeuverGenerator/","text":"Manuever Generator Version 1.0 Developer: Meet Maratha 1. Summary This module is responsible for generation of OpenScenario files using OpenDrive (XODR) map, Maneuver type, and the number of Vehicles as its input. The OpenDrive map can either be provided by the user, or can be generated using our module for OpenDrive file generator based on real-world OpenStreetMap (OSM) called OSM2XODR . These scenarios are generated around an Ego vehicle, which is basically the vehicle that is performing the maneuver. In it we compute relevant Region Of Interests (ROIs) for Ego vehicle, Helper vehicles, Non-ego vehicles, and additional objects and place them accordingly on the map. Based on the maneuver type we may require to control some other vehicles to garuntee that the maneuver happens, we rightfully call them Helper vehicles. The vehicles other than Ego vehicle and Helper Vehicles are classified as Non-ego vehicles. These vehicles do not directly take part in the maneuver but help to populate the environment. For some maneuvers such as Barricade maneuver, we require some barriers to be placed on the road. These are the additional objects which place based on the need of the maneuver. To each dynamic object in the maneuver we assign their relevant actions which they need to perform for generating the maneuver. This module uses OpenScenario 1.2.0 as its output format for the scenario. Before saving the scenario in database we perform a syntax check on it to ensure that there is no error in the generate OpenScenario file. We can use this generated OpenScenario file defining the maneuver in combination with our Variation Engine which can perform Permutation and Combination to generate variations of this base scenario to provide you bulk of OpenScenario files for one/collection of Base scenario(s). 2. Method 2.1 Inputs HD Map: This is the Road network on which we wish to generate the Scenario Maneuver Type: This is the type of maneuver we wish to generate. This is selected based on available maneuver types. Following maneuver types are available: Speeding Braking Lane Change Cut-in Cut-out Overtake Low Fuel Parking Pedestrian Crossing Emergency Vehicle Pass Barricade Number of Vehicles: This is the number of vehicles in the scenario including the Ego vehicle. 2.2 ROI Extractor Based on the HD Map and Maneuver type it first checks if the selected maneuver can be performed for this HD Map. If it cannot generate the maneuver on this map, it notifies the user and moves onto to another HD Map if available. If it can generate the maneuver on the provided HD Map, it extract spawn points for the Ego vehicle which can ensure that the maneuver occurs. It then checks if there is a need for Helper vehicles. As explained in the summary Helper vehicles are vehicles that help Ego vehicle to perform a certain type of maneuver. If there are Helper vehicles needed, it finds their possible spawn points based on the Ego vehicle position. After computation of all these, it computes spawn points for Non-ego vehicles. As defined in the summary these are vehicles that help generate environment beside the Ego vehicle. For their positions we find spawn points around Ego vehicle in a circle of radius of 300 meters. After it has generated these spawn points it provides us with python generator functions for each of these cases which we then attach to specific vehicles. We use all these ROIs till we reach the maximum number of files required. We assume an ROI has been used completely used when we have exhausted all the possibilities for Non-ego vehicle spawn points. 2.3 Ego position This is one of the ego position that has been chosen from the list of possible spawn points for the Ego vehicle based on the Maneuver type and the HD Map. 2.4 Helper position This is one of the helper position that has been chosen from the list of possible spawn points for the Helper vehicle based on the Maneuver type, HD Map, and Ego position. 2.5 Non-ego positions This is a list of required Non-ego positions based on the number of vehicles, Ego position, and HD Map provided as input. It is chosen from the complete list of possible Non-ego positions based on the Maneuver type, HD Map, and Ego position. 2.6 OpenScenario Initalizer This initalizes a basic OpenScenario file. For this we first place all the vehicles based on the Ego position, Helper position and Non-ego positions. After this we assign speed for Ego and Non-ego vehicles. We compute the speed of Helper vehicle based on the speed of Ego vehicle and maneuver type. 2.7 Base OpenScenario with Initial events Anything that happens in an OpenScenario file is called an event. We get a basic OpenScenario file at this stage which has all the initial events and vehicles in it. 2.8 Event Generator Inputs: Base OpenScenario file with Initial events Ego position Helper position Non-ego vehicles Maneuver type In it we generate all the the additional events that are necessary for this maneuver to be performed. All this updation is performed on the Base OpenScenario file with Initial events. We require Ego position and Helper position to generate maneuver events. We require Non-ego vehicles to specify any special events that might need to happen when Ego vehicle interacts with them. We require Maneuver type to define the event that needs to be generated using Ego and Helper vehicle. 2.9 OpenScenario with Maneuver events At this stage we have an OpenScenario file with all the events defined. 2.10 Additional objects These are the additional objects that neeeds to be placed in the environment for a certain maneuver to be performed. For example, to complete the barriaced maneuver, we need barricade to be placed on the lane on which Ego vehicle is travelling. 2.11 Object Placer This places all the required objects in the OpenScenario file. 2.12 OpenScenario Datastructure This is the OpenScenario datastructure that is ready for writing to a file. 2.13 OpenScenario File Writer This module writes the OpenScenario datastrucure in OpenScenario 1.2.0 file format. 2.14 OpenScenario File This is the final OpenScenario file that performs the selected maneuver on the specified HD Map. 2.15 Syntax Checker This module checks for any error in the syntax of the generated OpenScenario file. If there is some error in the generated OpenScenario file, it deletes that file and moves on to another process. 2.16 Output Our output is the base scenario maneuver file which will perform a specified maneuver on the selected HD map when it is ran. It is stored in the database for performing some other computations based on this file. Requirement Installed Python version 3.10 ScenarioGeneration Python package.","title":"Manuever Generator"},{"location":"ManeuverGenerator/#manuever-generator","text":"","title":"Manuever Generator"},{"location":"ManeuverGenerator/#version-10","text":"","title":"Version 1.0"},{"location":"ManeuverGenerator/#developer-meet-maratha","text":"","title":"Developer: Meet Maratha"},{"location":"ManeuverGenerator/#1-summary","text":"This module is responsible for generation of OpenScenario files using OpenDrive (XODR) map, Maneuver type, and the number of Vehicles as its input. The OpenDrive map can either be provided by the user, or can be generated using our module for OpenDrive file generator based on real-world OpenStreetMap (OSM) called OSM2XODR . These scenarios are generated around an Ego vehicle, which is basically the vehicle that is performing the maneuver. In it we compute relevant Region Of Interests (ROIs) for Ego vehicle, Helper vehicles, Non-ego vehicles, and additional objects and place them accordingly on the map. Based on the maneuver type we may require to control some other vehicles to garuntee that the maneuver happens, we rightfully call them Helper vehicles. The vehicles other than Ego vehicle and Helper Vehicles are classified as Non-ego vehicles. These vehicles do not directly take part in the maneuver but help to populate the environment. For some maneuvers such as Barricade maneuver, we require some barriers to be placed on the road. These are the additional objects which place based on the need of the maneuver. To each dynamic object in the maneuver we assign their relevant actions which they need to perform for generating the maneuver. This module uses OpenScenario 1.2.0 as its output format for the scenario. Before saving the scenario in database we perform a syntax check on it to ensure that there is no error in the generate OpenScenario file. We can use this generated OpenScenario file defining the maneuver in combination with our Variation Engine which can perform Permutation and Combination to generate variations of this base scenario to provide you bulk of OpenScenario files for one/collection of Base scenario(s).","title":"1. Summary"},{"location":"ManeuverGenerator/#2-method","text":"","title":"2. Method"},{"location":"ManeuverGenerator/#21-inputs","text":"HD Map: This is the Road network on which we wish to generate the Scenario Maneuver Type: This is the type of maneuver we wish to generate. This is selected based on available maneuver types. Following maneuver types are available: Speeding Braking Lane Change Cut-in Cut-out Overtake Low Fuel Parking Pedestrian Crossing Emergency Vehicle Pass Barricade Number of Vehicles: This is the number of vehicles in the scenario including the Ego vehicle.","title":"2.1 Inputs"},{"location":"ManeuverGenerator/#22-roi-extractor","text":"Based on the HD Map and Maneuver type it first checks if the selected maneuver can be performed for this HD Map. If it cannot generate the maneuver on this map, it notifies the user and moves onto to another HD Map if available. If it can generate the maneuver on the provided HD Map, it extract spawn points for the Ego vehicle which can ensure that the maneuver occurs. It then checks if there is a need for Helper vehicles. As explained in the summary Helper vehicles are vehicles that help Ego vehicle to perform a certain type of maneuver. If there are Helper vehicles needed, it finds their possible spawn points based on the Ego vehicle position. After computation of all these, it computes spawn points for Non-ego vehicles. As defined in the summary these are vehicles that help generate environment beside the Ego vehicle. For their positions we find spawn points around Ego vehicle in a circle of radius of 300 meters. After it has generated these spawn points it provides us with python generator functions for each of these cases which we then attach to specific vehicles. We use all these ROIs till we reach the maximum number of files required. We assume an ROI has been used completely used when we have exhausted all the possibilities for Non-ego vehicle spawn points.","title":"2.2 ROI Extractor"},{"location":"ManeuverGenerator/#23-ego-position","text":"This is one of the ego position that has been chosen from the list of possible spawn points for the Ego vehicle based on the Maneuver type and the HD Map.","title":"2.3 Ego position"},{"location":"ManeuverGenerator/#24-helper-position","text":"This is one of the helper position that has been chosen from the list of possible spawn points for the Helper vehicle based on the Maneuver type, HD Map, and Ego position.","title":"2.4 Helper position"},{"location":"ManeuverGenerator/#25-non-ego-positions","text":"This is a list of required Non-ego positions based on the number of vehicles, Ego position, and HD Map provided as input. It is chosen from the complete list of possible Non-ego positions based on the Maneuver type, HD Map, and Ego position.","title":"2.5 Non-ego positions"},{"location":"ManeuverGenerator/#26-openscenario-initalizer","text":"This initalizes a basic OpenScenario file. For this we first place all the vehicles based on the Ego position, Helper position and Non-ego positions. After this we assign speed for Ego and Non-ego vehicles. We compute the speed of Helper vehicle based on the speed of Ego vehicle and maneuver type.","title":"2.6 OpenScenario Initalizer"},{"location":"ManeuverGenerator/#27-base-openscenario-with-initial-events","text":"Anything that happens in an OpenScenario file is called an event. We get a basic OpenScenario file at this stage which has all the initial events and vehicles in it.","title":"2.7 Base OpenScenario with Initial events"},{"location":"ManeuverGenerator/#28-event-generator","text":"Inputs: Base OpenScenario file with Initial events Ego position Helper position Non-ego vehicles Maneuver type In it we generate all the the additional events that are necessary for this maneuver to be performed. All this updation is performed on the Base OpenScenario file with Initial events. We require Ego position and Helper position to generate maneuver events. We require Non-ego vehicles to specify any special events that might need to happen when Ego vehicle interacts with them. We require Maneuver type to define the event that needs to be generated using Ego and Helper vehicle.","title":"2.8 Event Generator"},{"location":"ManeuverGenerator/#29-openscenario-with-maneuver-events","text":"At this stage we have an OpenScenario file with all the events defined.","title":"2.9 OpenScenario with Maneuver events"},{"location":"ManeuverGenerator/#210-additional-objects","text":"These are the additional objects that neeeds to be placed in the environment for a certain maneuver to be performed. For example, to complete the barriaced maneuver, we need barricade to be placed on the lane on which Ego vehicle is travelling.","title":"2.10 Additional objects"},{"location":"ManeuverGenerator/#211-object-placer","text":"This places all the required objects in the OpenScenario file.","title":"2.11 Object Placer"},{"location":"ManeuverGenerator/#212-openscenario-datastructure","text":"This is the OpenScenario datastructure that is ready for writing to a file.","title":"2.12 OpenScenario Datastructure"},{"location":"ManeuverGenerator/#213-openscenario-file-writer","text":"This module writes the OpenScenario datastrucure in OpenScenario 1.2.0 file format.","title":"2.13 OpenScenario File Writer"},{"location":"ManeuverGenerator/#214-openscenario-file","text":"This is the final OpenScenario file that performs the selected maneuver on the specified HD Map.","title":"2.14 OpenScenario File"},{"location":"ManeuverGenerator/#215-syntax-checker","text":"This module checks for any error in the syntax of the generated OpenScenario file. If there is some error in the generated OpenScenario file, it deletes that file and moves on to another process.","title":"2.15 Syntax Checker"},{"location":"ManeuverGenerator/#216-output","text":"Our output is the base scenario maneuver file which will perform a specified maneuver on the selected HD map when it is ran. It is stored in the database for performing some other computations based on this file.","title":"2.16 Output"},{"location":"ManeuverGenerator/#requirement","text":"Installed Python version 3.10 ScenarioGeneration Python package.","title":"Requirement"},{"location":"OSM2XODR/","text":"OSM2XODR Version 1.0 Developer: Meet Maratha 1. Summary Needs updating This module is responsible for generation of a type of HD Map called OpenDrive (XODR) file based on real-world OpenStreetMap (OSM) data. This is used in tandem with our other modules to generate scenarios and sensor data for those scenarios. One of our module that can benefits a lot from this module is Maneuver Generator that uses an OpenDrive file with some other inputs to generate scenarios performing certain type of maneuver. It can also be used with our Traffic Manager module which can generate traffic on this generated OpenDrive file for generating sensor data. The inputs to this module is a text (.txt) file which consists of extents on OSM highlighting the required region. This required region is downloaded and road information such as road network and roadside objects are extracted from this OSM to generate OpenDrive file. After this OpenDrive file is generated a check is performed on it to verify if there is no syntax error in the file. 2. Method Needs updating 2.1 Inputs OSM Extents: This is a text file which contains extents of the regions we need to generate OpenDrive file for, from OpenStreetMap. 2.2 Extent Loader This reads the input text file and stores them as a list. 2.3 List of All extents This is a list of all the extents stored in software to be processed. 2.4 OpenStreetMap Downloader This processess each of the extent serially and downloads the respective OSM using wget. It stores the downloaded OSM in the relevant folder 2.5 OpenStreetMaps These are the downloaded OSMs. 2.6 Road Network Extractor This extracts and extrapolates road network such that we can create an OpenDrive map. To do this it makes use of Nodes and Ways in OSM, and extrapolates connections and geometry. This information is stored as a data structure 2.7 Road Network The data structure that holds information about all the roads in the road network. This is used to generate the OpenDrive file in further steps. 2.8 Roadside Objects Extractor Inputs: Road Network OpenStreetMap This uses the data in OSM to find road side objects such as traffic signal, signs, and zebra crossings in the network. It then uses Road Network to compute the local co-ordinate for these objects based on the road they are attached. 2.9 Roadside objects These are the local co-ordinate position for the roadside objects that is computed from OSM data. 2.10 OpenDrive Writer Inputs: Road Network Roadside Objects This writes the OpenDrive file based on the Road Network data structure and Roadside objects. 2.11 OpenDrive Maps This the generated OpenDrive file that is stored in the default folder. 2.12 Syntax Checker This module checks for any error in the syntax of the generated OpenDrive file. If there is some error in the generated OpenDrive file, it deletes that file and moves on to another process. 2.16 Output Our output is the OpenDrive file generated for the OSM extent. It is stored in the database for performing some other computations based on this file. Requirement Installed Python version 3.10 osmread package. Installed wget","title":"OSM To XODR"},{"location":"OSM2XODR/#osm2xodr","text":"","title":"OSM2XODR"},{"location":"OSM2XODR/#version-10","text":"","title":"Version 1.0"},{"location":"OSM2XODR/#developer-meet-maratha","text":"","title":"Developer: Meet Maratha"},{"location":"OSM2XODR/#1-summary","text":"","title":"1. Summary"},{"location":"OSM2XODR/#needs-updating","text":"This module is responsible for generation of a type of HD Map called OpenDrive (XODR) file based on real-world OpenStreetMap (OSM) data. This is used in tandem with our other modules to generate scenarios and sensor data for those scenarios. One of our module that can benefits a lot from this module is Maneuver Generator that uses an OpenDrive file with some other inputs to generate scenarios performing certain type of maneuver. It can also be used with our Traffic Manager module which can generate traffic on this generated OpenDrive file for generating sensor data. The inputs to this module is a text (.txt) file which consists of extents on OSM highlighting the required region. This required region is downloaded and road information such as road network and roadside objects are extracted from this OSM to generate OpenDrive file. After this OpenDrive file is generated a check is performed on it to verify if there is no syntax error in the file.","title":"Needs updating"},{"location":"OSM2XODR/#2-method","text":"","title":"2. Method"},{"location":"OSM2XODR/#needs-updating_1","text":"","title":"Needs updating"},{"location":"OSM2XODR/#21-inputs","text":"OSM Extents: This is a text file which contains extents of the regions we need to generate OpenDrive file for, from OpenStreetMap.","title":"2.1 Inputs"},{"location":"OSM2XODR/#22-extent-loader","text":"This reads the input text file and stores them as a list.","title":"2.2 Extent Loader"},{"location":"OSM2XODR/#23-list-of-all-extents","text":"This is a list of all the extents stored in software to be processed.","title":"2.3 List of All extents"},{"location":"OSM2XODR/#24-openstreetmap-downloader","text":"This processess each of the extent serially and downloads the respective OSM using wget. It stores the downloaded OSM in the relevant folder","title":"2.4 OpenStreetMap Downloader"},{"location":"OSM2XODR/#25-openstreetmaps","text":"These are the downloaded OSMs.","title":"2.5 OpenStreetMaps"},{"location":"OSM2XODR/#26-road-network-extractor","text":"This extracts and extrapolates road network such that we can create an OpenDrive map. To do this it makes use of Nodes and Ways in OSM, and extrapolates connections and geometry. This information is stored as a data structure","title":"2.6 Road Network Extractor"},{"location":"OSM2XODR/#27-road-network","text":"The data structure that holds information about all the roads in the road network. This is used to generate the OpenDrive file in further steps.","title":"2.7 Road Network"},{"location":"OSM2XODR/#28-roadside-objects-extractor","text":"Inputs: Road Network OpenStreetMap This uses the data in OSM to find road side objects such as traffic signal, signs, and zebra crossings in the network. It then uses Road Network to compute the local co-ordinate for these objects based on the road they are attached.","title":"2.8 Roadside Objects Extractor"},{"location":"OSM2XODR/#29-roadside-objects","text":"These are the local co-ordinate position for the roadside objects that is computed from OSM data.","title":"2.9 Roadside objects"},{"location":"OSM2XODR/#210-opendrive-writer","text":"Inputs: Road Network Roadside Objects This writes the OpenDrive file based on the Road Network data structure and Roadside objects.","title":"2.10 OpenDrive Writer"},{"location":"OSM2XODR/#211-opendrive-maps","text":"This the generated OpenDrive file that is stored in the default folder.","title":"2.11 OpenDrive Maps"},{"location":"OSM2XODR/#212-syntax-checker","text":"This module checks for any error in the syntax of the generated OpenDrive file. If there is some error in the generated OpenDrive file, it deletes that file and moves on to another process.","title":"2.12 Syntax Checker"},{"location":"OSM2XODR/#216-output","text":"Our output is the OpenDrive file generated for the OSM extent. It is stored in the database for performing some other computations based on this file.","title":"2.16 Output"},{"location":"OSM2XODR/#requirement","text":"Installed Python version 3.10 osmread package. Installed wget","title":"Requirement"},{"location":"PedestrianManager/","text":"Pedestrian Manager Version Developer 1. Summary This module is designed to control the pedestrians in simulation. This module controls the movement of pedestrian and ensues that they only move in pedestrian area. 2. Method At the beginning of the simulation, spawn points are calculated for pedestrians. After generating these spawn points, a random point is selected on the navigational mesh within a radius of 5m around each spawn point. Pedestrians are then spawned at the resulting point. Once a pedestrian is spawned it is attached to a custom controller AIPedestrain Controller which controles the movement of spawned pedestrians. This controller then finds the next target location for pedestrian. The steps follow to find target poits are: 1. To determine the target location for pedestrians, non-collision boxes with specific materials are positioned on a spline, spaced at intervals of 4m from each other. These boxes serve as reference points for tracing and locating the target destination. 2. Pedestrians initiate a sphere trace in 6m front of them, spanning a distance of 5m, to identify these boxes, referred to as hit points. Upon identifying a hit point, the pedestrian proceeds to search for a target location on the navigational mesh within a radius of 1m around each hit point and move to that target location. 3. In instances where no hit point is initially detected, the pedestrian rotates by 30 degrees to continue the search for hit points. If a hit point is found directly ahead but no navigational point is available on the nav mesh, the pedestrian rotates by an additional 30 degrees and continues the search for alternative hit points. This rotation and search process persists until a suitable hit point and navigational point are identified. If pedestrian completes a full 360 rotation it stops moving. Pedestrian attributes Input attribute Type Default Description numPedestrian int 50 Number of pedestrian to spawn. 3. How To How to add a new pedestrian","title":"Pedestrian"},{"location":"PedestrianManager/#pedestrian-manager","text":"","title":"Pedestrian Manager"},{"location":"PedestrianManager/#version","text":"","title":"Version"},{"location":"PedestrianManager/#developer","text":"","title":"Developer"},{"location":"PedestrianManager/#1-summary","text":"This module is designed to control the pedestrians in simulation. This module controls the movement of pedestrian and ensues that they only move in pedestrian area.","title":"1. Summary"},{"location":"PedestrianManager/#2-method","text":"At the beginning of the simulation, spawn points are calculated for pedestrians. After generating these spawn points, a random point is selected on the navigational mesh within a radius of 5m around each spawn point. Pedestrians are then spawned at the resulting point. Once a pedestrian is spawned it is attached to a custom controller AIPedestrain Controller which controles the movement of spawned pedestrians. This controller then finds the next target location for pedestrian. The steps follow to find target poits are: 1. To determine the target location for pedestrians, non-collision boxes with specific materials are positioned on a spline, spaced at intervals of 4m from each other. These boxes serve as reference points for tracing and locating the target destination. 2. Pedestrians initiate a sphere trace in 6m front of them, spanning a distance of 5m, to identify these boxes, referred to as hit points. Upon identifying a hit point, the pedestrian proceeds to search for a target location on the navigational mesh within a radius of 1m around each hit point and move to that target location. 3. In instances where no hit point is initially detected, the pedestrian rotates by 30 degrees to continue the search for hit points. If a hit point is found directly ahead but no navigational point is available on the nav mesh, the pedestrian rotates by an additional 30 degrees and continues the search for alternative hit points. This rotation and search process persists until a suitable hit point and navigational point are identified. If pedestrian completes a full 360 rotation it stops moving.","title":"2. Method"},{"location":"PedestrianManager/#pedestrian-attributes","text":"Input attribute Type Default Description numPedestrian int 50 Number of pedestrian to spawn.","title":"Pedestrian attributes"},{"location":"PedestrianManager/#3-how-to","text":"How to add a new pedestrian","title":"3. How To"},{"location":"PedestrianModelling/","text":"Dynamic Pedestrians Version 1.0 Modeler - Manasvi Kale 1. Summary This Document aims at the creation of the pedestrian blueprints in alignment with the pedestrian manager module. These pedestrians blueprints depicts the human behaviour during simulation. They can walk on footpaths, crosswalks, follows traffic lights, and can perform diverse actions such as running, jogging, walking, standing idle, sitting, and driving vehicles. The asset library consists of 19 pedestrain assets and each of them has 6 animations respectively. 2. Method Dynamic Pedestrian module consits of 7 different components which requires to be set up independently inorder for the whole module to work properly. Refer the how to guide set these components in alignment with each other. Flowchart - 3. Requirements Character Skeletal Mesh Character Animations 1D Blend Space Animation Blueprints Nav Mesh Nav Mesh Modifier Pedestrian Path Blueprints 4. How-to-guide 4.1 Step 1 - Creating 1D Blend Space Step 1 - Create a 1D BlendSpace and name it \u201cname_anime_BS\u201d, while creating this select your character skeleton in it. Step 2 - Inside Blendspace rename horizontal axis as \u201cspeed\u201d Step 3 - Set max speed as 140 Step 4 - Add breathing idle in the bottom grid followed by walking idle 4 times in the same grid. Step 5 - Save and close. 4.2 Step 2 - Creating AnimationBP for the same character. Step 1 - Create an animation Blueprint from the drop-down that pops up after clicking right in the content folder. Step 2 - While you create AnimBP it will ask for a target skeleton, select your character skeleton in that. Step 3 - In anim graph add a new state machine, rename it to \u201ccharactername state machine\u201d. Step 4 - Double click and open state machine Step 5 - Take the arrow from entry and add a new state, rename it to idle. Step 6 - From idle state add a new state and rename it to walk. Step 7 - Select transition node and set duration value to \u201c0.5\u201d. (For all the transition nodes) Step 8 - Open the states added in earlier (i.e idle and walk) Step 9 - Inside the idle state drag and drop idle animation from asset browser. Step 10 - Join the animation node to the output animation node. Step 11 - Now open walk state and drag and drop Blendspace that was created earlier Step 12 - Right click on speed and promote it to variable Step 13 - Now open idle to walk transition state 13.1 Drag speed from variable and add float greater than node, put value 0 and join it to result node. Step 14 - Open walk to idle transition drag speed variable in it, add float < float node and add value as 0.001, connect it with output. Step 15 - Open the event graph for animBP and add nodes shown in image below. Image insertion - event graph of animationBP Step 16 - Compile and save Step 17 - Finally, open the pedestrian BP , add this anim BP along with skeletal mesh and save it. 4.3 Step 3 - Nav Mesh Step 1 - Place nav mesh and scale it for the entire map and adjust it to be applicable only on all places pedestrians will move. 4.4 Step 4 - Nav Mesh Modifier Step 1 - If Nav mesh is covers the map including road as well. Put a Nav mesh modifier on the road and scale it, don\u2019t cover crosswalks in it. 4.5 Step 5 - Pedestrian Paths Step 1 - The pedestrian path spline BP component is placed in the pedestrains folder. Place the spline and extend it throughout the map at all places where pedestrains are expected to spawn.","title":"Pedestrians"},{"location":"PedestrianModelling/#dynamic-pedestrians","text":"","title":"Dynamic Pedestrians"},{"location":"PedestrianModelling/#version-10","text":"","title":"Version 1.0"},{"location":"PedestrianModelling/#modeler-manasvi-kale","text":"","title":"Modeler - Manasvi Kale"},{"location":"PedestrianModelling/#1-summary","text":"This Document aims at the creation of the pedestrian blueprints in alignment with the pedestrian manager module. These pedestrians blueprints depicts the human behaviour during simulation. They can walk on footpaths, crosswalks, follows traffic lights, and can perform diverse actions such as running, jogging, walking, standing idle, sitting, and driving vehicles. The asset library consists of 19 pedestrain assets and each of them has 6 animations respectively.","title":"1. Summary"},{"location":"PedestrianModelling/#2-method","text":"Dynamic Pedestrian module consits of 7 different components which requires to be set up independently inorder for the whole module to work properly. Refer the how to guide set these components in alignment with each other. Flowchart -","title":"2. Method"},{"location":"PedestrianModelling/#3-requirements","text":"Character Skeletal Mesh Character Animations 1D Blend Space Animation Blueprints Nav Mesh Nav Mesh Modifier Pedestrian Path Blueprints","title":"3. Requirements"},{"location":"PedestrianModelling/#4-how-to-guide","text":"","title":"4. How-to-guide"},{"location":"PedestrianModelling/#41-step-1-creating-1d-blend-space","text":"Step 1 - Create a 1D BlendSpace and name it \u201cname_anime_BS\u201d, while creating this select your character skeleton in it. Step 2 - Inside Blendspace rename horizontal axis as \u201cspeed\u201d Step 3 - Set max speed as 140 Step 4 - Add breathing idle in the bottom grid followed by walking idle 4 times in the same grid. Step 5 - Save and close.","title":"4.1 Step 1 - Creating 1D Blend Space"},{"location":"PedestrianModelling/#42-step-2-creating-animationbp-for-the-same-character","text":"Step 1 - Create an animation Blueprint from the drop-down that pops up after clicking right in the content folder. Step 2 - While you create AnimBP it will ask for a target skeleton, select your character skeleton in that. Step 3 - In anim graph add a new state machine, rename it to \u201ccharactername state machine\u201d. Step 4 - Double click and open state machine Step 5 - Take the arrow from entry and add a new state, rename it to idle. Step 6 - From idle state add a new state and rename it to walk. Step 7 - Select transition node and set duration value to \u201c0.5\u201d. (For all the transition nodes) Step 8 - Open the states added in earlier (i.e idle and walk) Step 9 - Inside the idle state drag and drop idle animation from asset browser. Step 10 - Join the animation node to the output animation node. Step 11 - Now open walk state and drag and drop Blendspace that was created earlier Step 12 - Right click on speed and promote it to variable Step 13 - Now open idle to walk transition state 13.1 Drag speed from variable and add float greater than node, put value 0 and join it to result node. Step 14 - Open walk to idle transition drag speed variable in it, add float < float node and add value as 0.001, connect it with output. Step 15 - Open the event graph for animBP and add nodes shown in image below. Image insertion - event graph of animationBP Step 16 - Compile and save Step 17 - Finally, open the pedestrian BP , add this anim BP along with skeletal mesh and save it.","title":"4.2 Step 2 - Creating AnimationBP for the same character."},{"location":"PedestrianModelling/#43-step-3-nav-mesh","text":"Step 1 - Place nav mesh and scale it for the entire map and adjust it to be applicable only on all places pedestrians will move.","title":"4.3 Step 3 -  Nav Mesh"},{"location":"PedestrianModelling/#44-step-4-nav-mesh-modifier","text":"Step 1 - If Nav mesh is covers the map including road as well. Put a Nav mesh modifier on the road and scale it, don\u2019t cover crosswalks in it.","title":"4.4 Step 4 -  Nav Mesh Modifier"},{"location":"PedestrianModelling/#45-step-5-pedestrian-paths","text":"Step 1 - The pedestrian path spline BP component is placed in the pedestrains folder. Place the spline and extend it throughout the map at all places where pedestrains are expected to spawn.","title":"4.5 Step 5 -  Pedestrian Paths"},{"location":"Radar/","text":"Module Name Radar Version Developer Shirshakk Purkayastha 1. Summary The Radar sensor is used to generate point sized rays using the input parameters of sensors {range, HorizontalFOV, VerticalFOV, and PulsesPerSeconds} to stimulate an automobile radar sensor. It takes into account the FOV parameters (horizontal and vertical), creates a cone of view on this basis, uses raycasting to stimulate radar waves and provides the return information of the detected objects. The output includes the details of coordinates, range, relative velocity, azimuth and elevatiion angles of each hit. 2. Method Using the input parameters of range, HorizontalFOV, and VerticalFOV, a cone of interest is created in front of the radar sensor. Within this cone, 'PPS' number of rays are fired uniformly in random (Ray Casting), taking into account the stimulation speed (FPS). Whenever the points touch down for the first instance within the range, a hit is generated by using the LineTraceSingleByChannel function of Unreal read_more . The hit is stored in an OutHit data structure and corresponding details of [x,y,z], relative_velocity, azimuth_angle, vertical_angle, and range are then calcualted and stored into the OutHit structure. Data Flow Diagram: Radar attributes: Input Attributes Type Default Value Description Range float 20000 Maximum distance to which the pulses can return back to the radar sensor after being fired HorizontalFOV float 120 Total coverage angle of the radar sensor in the horizontal plane VerticalFOV float 30 Toatl coverage angle of the radar sensor in the vertical plane PPS int 1500 Pulses Per Second, denotes the number of pulses to be fired in one second Output Attributes Type Description X float X- coordinate of the point where the radar ray hits Y float Y- coordinate of the point where the radar ray hits Z float Z- coordinate of the point where the radar ray hits velocity float Relative velocity of the detected object azimuth_angle float Horizontal angle of the detected object w.r.t. the radar elevation_angle float Vertical angle of the detected object w.r.t. the radar range float Distance of the detected object from the radar sensor Output: The [x,y,z] coordinates of the detection, along with relative velocity, range, and the corresponding horizontal and vertical angles of the detection, stored in the Rays data structure. 3. Requirements The super file containing the details of each sensor (Sensor.h).","title":"Radar"},{"location":"Radar/#module-name","text":"Radar","title":"Module Name"},{"location":"Radar/#version","text":"","title":"Version"},{"location":"Radar/#developer","text":"Shirshakk Purkayastha","title":"Developer"},{"location":"Radar/#1-summary","text":"The Radar sensor is used to generate point sized rays using the input parameters of sensors {range, HorizontalFOV, VerticalFOV, and PulsesPerSeconds} to stimulate an automobile radar sensor. It takes into account the FOV parameters (horizontal and vertical), creates a cone of view on this basis, uses raycasting to stimulate radar waves and provides the return information of the detected objects. The output includes the details of coordinates, range, relative velocity, azimuth and elevatiion angles of each hit.","title":"1. Summary"},{"location":"Radar/#2-method","text":"Using the input parameters of range, HorizontalFOV, and VerticalFOV, a cone of interest is created in front of the radar sensor. Within this cone, 'PPS' number of rays are fired uniformly in random (Ray Casting), taking into account the stimulation speed (FPS). Whenever the points touch down for the first instance within the range, a hit is generated by using the LineTraceSingleByChannel function of Unreal read_more . The hit is stored in an OutHit data structure and corresponding details of [x,y,z], relative_velocity, azimuth_angle, vertical_angle, and range are then calcualted and stored into the OutHit structure.","title":"2. Method"},{"location":"Radar/#data-flow-diagram","text":"","title":"Data Flow Diagram:"},{"location":"Radar/#radar-attributes","text":"Input Attributes Type Default Value Description Range float 20000 Maximum distance to which the pulses can return back to the radar sensor after being fired HorizontalFOV float 120 Total coverage angle of the radar sensor in the horizontal plane VerticalFOV float 30 Toatl coverage angle of the radar sensor in the vertical plane PPS int 1500 Pulses Per Second, denotes the number of pulses to be fired in one second Output Attributes Type Description X float X- coordinate of the point where the radar ray hits Y float Y- coordinate of the point where the radar ray hits Z float Z- coordinate of the point where the radar ray hits velocity float Relative velocity of the detected object azimuth_angle float Horizontal angle of the detected object w.r.t. the radar elevation_angle float Vertical angle of the detected object w.r.t. the radar range float Distance of the detected object from the radar sensor","title":"Radar attributes:"},{"location":"Radar/#output","text":"The [x,y,z] coordinates of the detection, along with relative velocity, range, and the corresponding horizontal and vertical angles of the detection, stored in the Rays data structure.","title":"Output:"},{"location":"Radar/#3-requirements","text":"The super file containing the details of each sensor (Sensor.h).","title":"3. Requirements"},{"location":"SemanticSegmentationCamera/","text":"Module Name Semantic Segmentation Camera Version Developer 1. Summary The Semantic segmentation camera sensor is designed to capture scenes and generate segmentated images in simulation. 2. Method Initally, a new 2D TextureRenderTarget object is created to render the target texture. The size and format of the pixels for the target texture are initialised according to sensor input, with the texture width and height set to the input imageWidth and imageHeight , respectively. Following the initialization of the target texture,the scene is captured using the SceneCaptureComponent provided by unreal engine. The rate at which scene is captured is determined by 1/scanningFrequency . Once a scene has been captured a Semantic segmentation material is applied on target texture. The Semantic segmentation material calutetes the segmented colour according to object class for each pixel. This resulting segmented color data is then send to an image data worker, which convets it into an segmented image in JPG format. The class-tag mapping with semantic color is shown in Tabel below- Tag Class Color 1 BoundaryFencing FColor{112, 160, 160} 2 Buildings FColor{116, 116, 116} 3 BusStand FColor{207, 207, 111} 4 Decals FColor{178, 139, 66} 5 Fences FColor{153, 153, 187} 6 Pedestrians FColor{228, 79, 109} 7 Props FColor{234, 175, 2} 8 RailTrack FColor{230, 150, 140} 9 RoadLaneMarkings FColor{196, 199, 198} 10 RoadMeshes FColor{160, 112, 160} 11 RoadsideFurniture FColor{220, 151, 245} 12 SideWalk FColor{246, 90, 236} 13 StreetLights FColor{178, 178, 178} 14 Terrain FColor{178, 252, 177} 15 TrafficSigns FColor{229, 227, 64} 16 TransmissionTower FColor{178, 178, 178} 17 Vegetation FColor{145, 170, 89} 18 Vehicles FColor{63, 64, 170} 19 TrafficLights FColor{181, 235, 231} 20 SmallPole FColor{178, 178, 178} 21 Sky FColor{116, 162, 198} 22 Rider FColor{254, 64, 66} 23 Truck FColor{64, 64, 116} 24 Bus FColor{64, 109, 140} 25 Train FColor{65, 124, 140} 26 Motorcycle FColor{64, 64, 236} 27 Bicycle FColor{153, 72, 87} 28 Bridge FColor{175, 139, 139} 29 Tunnel FColor{178, 153, 131} Camera attributes Input attribute Type Default Description imageWidth int 640 Image width in pixels imageHeight int 480 Image width in pixels scanningFrequency int 10 Capturing frequency of camera in Hz. bShow2DBounds bool false Whether to show 2D Bounding boxes in captured image. bShow3DBounds bool false Whether to show 3D Bounding boxes in captured image. bShowOccludedObjects bool false Whether to show Bounding boxes for occluded objects. bEnablePostProcessingEffects bool false Enable post-processing effects. targetGamma float 2.2 Target gamma value of the camera. fovAngle float 46.0 Horizontal field of view in degrees. shutterSpeed float 1/80.0 The camera shutter speed in seconds. ISO float 200 The camera sensor sensitivity. fstop float 1.4 Opening of the camera lens. Aperture is 1/fstop with typical lens going down to f/1.2 (larger opening). Larger numbers will reduce the Depth of Field effect. FocalDistance float 1000 Distance at which the depth of field effect should be sharp. Measured in cm (UE units). Post-Processing attributes Input attribute Type Default Description bEnableDepthBlur bool false To enable Depth Blur (Depth of Field) post-process effect. depthBlurBladeCount int 5 The number of blades of the diaphragm within the lens between [4,16]. depthBlurAmount float 1.0 Depth blur km for 50% depthBlurRadius float 0.0 Depth blur radius in pixels at 1920x depthBlurMinFStop float 1.2 the maximum opening of the camera lens to control the curvature of blades of the diaphragm. lensFlareIntensity flaot 0.1 Intensity for the lens flare post-process effect, 0.0 for disabling it. lensFlareThreshold float 0.1 Minimum brightness the lens flare starts having effect. lensFlareBokehSize float 8.0 Size of the lens blur (in percent of the view width) that is done with the Bokeh texture. motionBlurIntensity float 0.45 Strength of motion blur [0,1]. 0.0 for disabling it. motionBlurMaxDistortion float 0.35 Max distortion caused by motion blur. Percentage of screen width. 0.0 for disabling it. motionBlurPerObjectSize float 0.1 The minimum projected screen radius for a primitive to be drawn in the velocity pass for motion blur consideration. The radius is a percentage of screen width. motionBlurTargetFPS int 0 Defines the target frames per second (fps) for motion blur. 0 to make motion blur dependent on the actual measured frame rate. bloomThreshold float -1 Minimum luminance the bloom starts having effect. bloomIntensity float 0.67 Intensity for the bloom post-process effect, 0.0 for disabling it. bEnableChromaticAberration bool 10000.0 To enable Chromatic Aberration post-process effect. ChromaticAberrationIntensity float 0.0 Scaling factor to control color shifting, more noticeable on the screen borders. ChromaticAberrationOffset float 0.0 Normalized distance to the center of the image where the effect takes place. Output attributes Captured segmented image of provided height and width in jpg format. 3. Requirements Unreal","title":"Semantic Segmentation Camera"},{"location":"SemanticSegmentationCamera/#module-name","text":"Semantic Segmentation Camera","title":"Module Name"},{"location":"SemanticSegmentationCamera/#version","text":"","title":"Version"},{"location":"SemanticSegmentationCamera/#developer","text":"","title":"Developer"},{"location":"SemanticSegmentationCamera/#1-summary","text":"The Semantic segmentation camera sensor is designed to capture scenes and generate segmentated images in simulation.","title":"1. Summary"},{"location":"SemanticSegmentationCamera/#2-method","text":"Initally, a new 2D TextureRenderTarget object is created to render the target texture. The size and format of the pixels for the target texture are initialised according to sensor input, with the texture width and height set to the input imageWidth and imageHeight , respectively. Following the initialization of the target texture,the scene is captured using the SceneCaptureComponent provided by unreal engine. The rate at which scene is captured is determined by 1/scanningFrequency . Once a scene has been captured a Semantic segmentation material is applied on target texture. The Semantic segmentation material calutetes the segmented colour according to object class for each pixel. This resulting segmented color data is then send to an image data worker, which convets it into an segmented image in JPG format. The class-tag mapping with semantic color is shown in Tabel below- Tag Class Color 1 BoundaryFencing FColor{112, 160, 160} 2 Buildings FColor{116, 116, 116} 3 BusStand FColor{207, 207, 111} 4 Decals FColor{178, 139, 66} 5 Fences FColor{153, 153, 187} 6 Pedestrians FColor{228, 79, 109} 7 Props FColor{234, 175, 2} 8 RailTrack FColor{230, 150, 140} 9 RoadLaneMarkings FColor{196, 199, 198} 10 RoadMeshes FColor{160, 112, 160} 11 RoadsideFurniture FColor{220, 151, 245} 12 SideWalk FColor{246, 90, 236} 13 StreetLights FColor{178, 178, 178} 14 Terrain FColor{178, 252, 177} 15 TrafficSigns FColor{229, 227, 64} 16 TransmissionTower FColor{178, 178, 178} 17 Vegetation FColor{145, 170, 89} 18 Vehicles FColor{63, 64, 170} 19 TrafficLights FColor{181, 235, 231} 20 SmallPole FColor{178, 178, 178} 21 Sky FColor{116, 162, 198} 22 Rider FColor{254, 64, 66} 23 Truck FColor{64, 64, 116} 24 Bus FColor{64, 109, 140} 25 Train FColor{65, 124, 140} 26 Motorcycle FColor{64, 64, 236} 27 Bicycle FColor{153, 72, 87} 28 Bridge FColor{175, 139, 139} 29 Tunnel FColor{178, 153, 131}","title":"2. Method"},{"location":"SemanticSegmentationCamera/#camera-attributes","text":"Input attribute Type Default Description imageWidth int 640 Image width in pixels imageHeight int 480 Image width in pixels scanningFrequency int 10 Capturing frequency of camera in Hz. bShow2DBounds bool false Whether to show 2D Bounding boxes in captured image. bShow3DBounds bool false Whether to show 3D Bounding boxes in captured image. bShowOccludedObjects bool false Whether to show Bounding boxes for occluded objects. bEnablePostProcessingEffects bool false Enable post-processing effects. targetGamma float 2.2 Target gamma value of the camera. fovAngle float 46.0 Horizontal field of view in degrees. shutterSpeed float 1/80.0 The camera shutter speed in seconds. ISO float 200 The camera sensor sensitivity. fstop float 1.4 Opening of the camera lens. Aperture is 1/fstop with typical lens going down to f/1.2 (larger opening). Larger numbers will reduce the Depth of Field effect. FocalDistance float 1000 Distance at which the depth of field effect should be sharp. Measured in cm (UE units).","title":"Camera attributes"},{"location":"SemanticSegmentationCamera/#post-processing-attributes","text":"Input attribute Type Default Description bEnableDepthBlur bool false To enable Depth Blur (Depth of Field) post-process effect. depthBlurBladeCount int 5 The number of blades of the diaphragm within the lens between [4,16]. depthBlurAmount float 1.0 Depth blur km for 50% depthBlurRadius float 0.0 Depth blur radius in pixels at 1920x depthBlurMinFStop float 1.2 the maximum opening of the camera lens to control the curvature of blades of the diaphragm. lensFlareIntensity flaot 0.1 Intensity for the lens flare post-process effect, 0.0 for disabling it. lensFlareThreshold float 0.1 Minimum brightness the lens flare starts having effect. lensFlareBokehSize float 8.0 Size of the lens blur (in percent of the view width) that is done with the Bokeh texture. motionBlurIntensity float 0.45 Strength of motion blur [0,1]. 0.0 for disabling it. motionBlurMaxDistortion float 0.35 Max distortion caused by motion blur. Percentage of screen width. 0.0 for disabling it. motionBlurPerObjectSize float 0.1 The minimum projected screen radius for a primitive to be drawn in the velocity pass for motion blur consideration. The radius is a percentage of screen width. motionBlurTargetFPS int 0 Defines the target frames per second (fps) for motion blur. 0 to make motion blur dependent on the actual measured frame rate. bloomThreshold float -1 Minimum luminance the bloom starts having effect. bloomIntensity float 0.67 Intensity for the bloom post-process effect, 0.0 for disabling it. bEnableChromaticAberration bool 10000.0 To enable Chromatic Aberration post-process effect. ChromaticAberrationIntensity float 0.0 Scaling factor to control color shifting, more noticeable on the screen borders. ChromaticAberrationOffset float 0.0 Normalized distance to the center of the image where the effect takes place.","title":"Post-Processing attributes"},{"location":"SemanticSegmentationCamera/#output-attributes","text":"Captured segmented image of provided height and width in jpg format.","title":"Output attributes"},{"location":"SemanticSegmentationCamera/#3-requirements","text":"Unreal","title":"3. Requirements"},{"location":"SolidStateLidarSensor/","text":"Module Name Solid-State LIDAR Sensor Version Developer 1. Summary The Lidar sensor is designed to simulate a Soild-state Lidar sensor using ray-casting. It utilizes Lidar parameters such as channels both horizontal and vertical, scanning frequency, field of view both horizontal and vertical, and range to generate a point cloud output. The resulting point cloud contains location, time, distance, intensity, label and scan angle information in ply format. 2. Method To simulate the rotating LIDAR sensor, the sensor is first initiallised by computing horizontal and vertical angle for each horizontal and veritcal channel/laser within there respective FOV. Ray-casting is then performed for each horizontal and veritical channel/laser with a rate of 1/frequency . The point cloud obtained from ray-casting contains information about the hit point's location (in xyz), time , distance , intensity , label and scan angle . Rest of all funcitionality is same as Rotating Lidar Sensor . Solid State Lidar attributes Input attribute Type Default Description channelsH int 128 Number of lasers in horizontal. channelsV int 32 Number of lasers in vertical. frequency float 20.0 Sensor frequency (in Hz). wavelength float 1.4 Wavelength at which solid-state Lidar sensor operates (in micrometer). minRange float 0.0 Minimum distance at which a point can be detected by the Lidar sensor (in cm). maxRange float 10000.0 Maximum distance at which a point can be detected by the Lidar sensor (in cm). negHorizontalFOV float -45.0 Minimum horizontal Angle (in degrees). posHorizontalFOV float 75.0 Maximum horizontal Angle (in degrees). negVerticalFOV float -10.0 Minimum vertical Angle (in degrees). posVerticalFOV float 20.0 Maximum horizontal Angle (in degrees). atmosphereAttenuationRate float 0.004 Coefficient that measures the solid state Lidar instensity loss per meter, Output attributes Sensor data attribute Type Description x float X coordinate of hit point. y float Y coordinate of hit point. z float Z coordinate of hit point. intenstity float Intensity of the reflected leaser from the hit point. time float Simulation time of the measurement in seconds since the beginning of the episode. label uint Label of hit point (see class-label mapping). r uint red value of the color of label. g uint green value of the color of label. b uint blue value of the color of label. scan_angle float Horizontal angle of the hit point. distance float Distance of hit point from Lidar sensor (im cm). 3. Requirements","title":"Solid State Lidar"},{"location":"SolidStateLidarSensor/#module-name","text":"Solid-State LIDAR Sensor","title":"Module Name"},{"location":"SolidStateLidarSensor/#version","text":"","title":"Version"},{"location":"SolidStateLidarSensor/#developer","text":"","title":"Developer"},{"location":"SolidStateLidarSensor/#1-summary","text":"The Lidar sensor is designed to simulate a Soild-state Lidar sensor using ray-casting. It utilizes Lidar parameters such as channels both horizontal and vertical, scanning frequency, field of view both horizontal and vertical, and range to generate a point cloud output. The resulting point cloud contains location, time, distance, intensity, label and scan angle information in ply format.","title":"1. Summary"},{"location":"SolidStateLidarSensor/#2-method","text":"To simulate the rotating LIDAR sensor, the sensor is first initiallised by computing horizontal and vertical angle for each horizontal and veritcal channel/laser within there respective FOV. Ray-casting is then performed for each horizontal and veritical channel/laser with a rate of 1/frequency . The point cloud obtained from ray-casting contains information about the hit point's location (in xyz), time , distance , intensity , label and scan angle . Rest of all funcitionality is same as Rotating Lidar Sensor .","title":"2. Method"},{"location":"SolidStateLidarSensor/#solid-state-lidar-attributes","text":"Input attribute Type Default Description channelsH int 128 Number of lasers in horizontal. channelsV int 32 Number of lasers in vertical. frequency float 20.0 Sensor frequency (in Hz). wavelength float 1.4 Wavelength at which solid-state Lidar sensor operates (in micrometer). minRange float 0.0 Minimum distance at which a point can be detected by the Lidar sensor (in cm). maxRange float 10000.0 Maximum distance at which a point can be detected by the Lidar sensor (in cm). negHorizontalFOV float -45.0 Minimum horizontal Angle (in degrees). posHorizontalFOV float 75.0 Maximum horizontal Angle (in degrees). negVerticalFOV float -10.0 Minimum vertical Angle (in degrees). posVerticalFOV float 20.0 Maximum horizontal Angle (in degrees). atmosphereAttenuationRate float 0.004 Coefficient that measures the solid state Lidar instensity loss per meter,","title":"Solid State Lidar attributes"},{"location":"SolidStateLidarSensor/#output-attributes","text":"Sensor data attribute Type Description x float X coordinate of hit point. y float Y coordinate of hit point. z float Z coordinate of hit point. intenstity float Intensity of the reflected leaser from the hit point. time float Simulation time of the measurement in seconds since the beginning of the episode. label uint Label of hit point (see class-label mapping). r uint red value of the color of label. g uint green value of the color of label. b uint blue value of the color of label. scan_angle float Horizontal angle of the hit point. distance float Distance of hit point from Lidar sensor (im cm).","title":"Output attributes"},{"location":"SolidStateLidarSensor/#3-requirements","text":"","title":"3. Requirements"},{"location":"ToolSummary/","text":"Tool Summary Version 1.0 Developer - Manasvi Kale 1. Summary The tool summary is a collective guide that instructs downloading, creating and modifiying and storage of assets such that they are compatible for any type of sensor data generation in limulator. Asset Library 1. Method Asset Library is a folder structure inside limulator software which was deliberately created for the storage of 3D models which can be directly used to populate environment for sensor data generation. Refer the flowchart below for the folder sturcture of library. 2. Requirements Folder Structure 4. How-to-guide The asset library was created and comes installed along with limulator software. The folder structure is not to be altered by any user or third-party as it is directly linked with the segmentation and dectection of objects when scenario runs in simulation. Any changes in structure will lead to incorrect output for semantic camera images in the data. Naming Convention For Assets 1. Method The naming convention guide is created to name assets and their respective materials such that they are in alignment with sensor data generation and intensity visualition. The naming convention is was created in collaboration of SOP for lablelling document, parameters required and standards followed in modeling industry. Refer the country specific naming convetion and examples for understanding. 2. How-to-guide The table below provides a country specific abbreviation that is to be written in prefix of asset name. India: IN United States of America: US United Kingdom: GB Canada: CA Australia: AU Germany: DE France: FR Japan: JP China: CN Brazil: BR Russia: RU South Africa: ZA Italy: IT Spain: ES Mexico: MX Argentina: AR New Zealand: NZ Sweden: SE Switzerland: CH Netherlands: NL Belgium: BE Norway: NO Singapore: SG Malaysia: MY Indonesia: ID 3. Naming Convention The naming of assets and materials is demonstrated through examples below. 3.1 Assets \"SM_Countrycode_Assetname_instanceid_SOPLabel example - IN_building_01_061 3.1 Materials and Instances \"M_type_assetname_instanceid_part_partinstaceid\" \"MI_materialtype_assetname\" example - 1. M_Steel_ElectricPole_01 2. MI_Steel_ElectricPole_01 Master Material & Instances 1. Method The objective of master materials and their instances is to create a base template with material properties of roughness, strength, metallic, base color, normal, and size. And to implement it on texture images for object material creation. 2. Requirements Texture images 1.1 base color 1.2 normal map 1.3 roughness. 3. How-to-guide Nodes for properties are connected and default values are set. Master material is saved and material instances are created. 3.1 Step 1 - Material graph Material graph is created by initializing the master material inside the unreal engine. Connect the texture nodes with each other and leave it to their default properties. 3.2 Step 2 - Material Instance After saving material, create a material instance for the same material in content browser by write clicking on the material 4. Tutorials Master Material - Material Instance - Dynamic Pedestrians 1. Method The aim is to create a Dynamic pedestrian blueprint which will be able to simulate different behaviours in a certain areas on the map. 2. Requirements Character Skeletal Mesh character Animations Material Blueprints Character Blueprints 3. How-to-guide 3.1 Step 1 - Character Blueprints Import the skeletal mesh of the character. Import the character animations in the same folder. Create a charcater blueprint and the name is \u201cBP_nameofcharacter\u201d. Add the imported animation to the blueprint of the character. Go to the event graph add the \u201cAIMoveTo\u201d node with the event begin to play, add a self-pawn actor, and create a destination variable. (These destination variables can be used to identify the locations on which pedestrians will move). Add as many AIMoveTo nodes as many location input you would like user to input. Refer the event graph image to connect the nodes. Insert a nav mesh to the map and expand it throughout the map. Apply and save BP. 4. Tutorials","title":"Tool Summary"},{"location":"ToolSummary/#tool-summary","text":"","title":"Tool Summary"},{"location":"ToolSummary/#version-10","text":"","title":"Version 1.0"},{"location":"ToolSummary/#developer-manasvi-kale","text":"","title":"Developer - Manasvi Kale"},{"location":"ToolSummary/#1-summary","text":"The tool summary is a collective guide that instructs downloading, creating and modifiying and storage of assets such that they are compatible for any type of sensor data generation in limulator.","title":"1. Summary"},{"location":"ToolSummary/#asset-library","text":"","title":"Asset Library"},{"location":"ToolSummary/#1-method","text":"Asset Library is a folder structure inside limulator software which was deliberately created for the storage of 3D models which can be directly used to populate environment for sensor data generation. Refer the flowchart below for the folder sturcture of library.","title":"1. Method"},{"location":"ToolSummary/#2-requirements","text":"Folder Structure","title":"2. Requirements"},{"location":"ToolSummary/#4-how-to-guide","text":"The asset library was created and comes installed along with limulator software. The folder structure is not to be altered by any user or third-party as it is directly linked with the segmentation and dectection of objects when scenario runs in simulation. Any changes in structure will lead to incorrect output for semantic camera images in the data.","title":"4. How-to-guide"},{"location":"ToolSummary/#naming-convention-for-assets","text":"","title":"Naming Convention For Assets"},{"location":"ToolSummary/#1-method_1","text":"The naming convention guide is created to name assets and their respective materials such that they are in alignment with sensor data generation and intensity visualition. The naming convention is was created in collaboration of SOP for lablelling document, parameters required and standards followed in modeling industry. Refer the country specific naming convetion and examples for understanding.","title":"1. Method"},{"location":"ToolSummary/#2-how-to-guide","text":"The table below provides a country specific abbreviation that is to be written in prefix of asset name. India: IN United States of America: US United Kingdom: GB Canada: CA Australia: AU Germany: DE France: FR Japan: JP China: CN Brazil: BR Russia: RU South Africa: ZA Italy: IT Spain: ES Mexico: MX Argentina: AR New Zealand: NZ Sweden: SE Switzerland: CH Netherlands: NL Belgium: BE Norway: NO Singapore: SG Malaysia: MY Indonesia: ID","title":"2. How-to-guide"},{"location":"ToolSummary/#3-naming-convention","text":"The naming of assets and materials is demonstrated through examples below.","title":"3. Naming Convention"},{"location":"ToolSummary/#31-assets","text":"\"SM_Countrycode_Assetname_instanceid_SOPLabel example - IN_building_01_061","title":"3.1 Assets"},{"location":"ToolSummary/#31-materials-and-instances","text":"\"M_type_assetname_instanceid_part_partinstaceid\" \"MI_materialtype_assetname\" example - 1. M_Steel_ElectricPole_01 2. MI_Steel_ElectricPole_01","title":"3.1 Materials and Instances"},{"location":"ToolSummary/#master-material-instances","text":"","title":"Master Material &amp; Instances"},{"location":"ToolSummary/#1-method_2","text":"The objective of master materials and their instances is to create a base template with material properties of roughness, strength, metallic, base color, normal, and size. And to implement it on texture images for object material creation.","title":"1. Method"},{"location":"ToolSummary/#2-requirements_1","text":"Texture images 1.1 base color 1.2 normal map 1.3 roughness.","title":"2. Requirements"},{"location":"ToolSummary/#3-how-to-guide","text":"Nodes for properties are connected and default values are set. Master material is saved and material instances are created.","title":"3. How-to-guide"},{"location":"ToolSummary/#31-step-1-material-graph","text":"Material graph is created by initializing the master material inside the unreal engine. Connect the texture nodes with each other and leave it to their default properties.","title":"3.1 Step 1 - Material graph"},{"location":"ToolSummary/#32-step-2-material-instance","text":"After saving material, create a material instance for the same material in content browser by write clicking on the material","title":"3.2 Step 2 -  Material Instance"},{"location":"ToolSummary/#4-tutorials","text":"Master Material - Material Instance -","title":"4. Tutorials"},{"location":"ToolSummary/#dynamic-pedestrians","text":"","title":"Dynamic Pedestrians"},{"location":"ToolSummary/#1-method_3","text":"The aim is to create a Dynamic pedestrian blueprint which will be able to simulate different behaviours in a certain areas on the map.","title":"1. Method"},{"location":"ToolSummary/#2-requirements_2","text":"Character Skeletal Mesh character Animations Material Blueprints Character Blueprints","title":"2. Requirements"},{"location":"ToolSummary/#3-how-to-guide_1","text":"","title":"3. How-to-guide"},{"location":"ToolSummary/#31-step-1-character-blueprints","text":"Import the skeletal mesh of the character. Import the character animations in the same folder. Create a charcater blueprint and the name is \u201cBP_nameofcharacter\u201d. Add the imported animation to the blueprint of the character. Go to the event graph add the \u201cAIMoveTo\u201d node with the event begin to play, add a self-pawn actor, and create a destination variable. (These destination variables can be used to identify the locations on which pedestrians will move). Add as many AIMoveTo nodes as many location input you would like user to input. Refer the event graph image to connect the nodes. Insert a nav mesh to the map and expand it throughout the map. Apply and save BP.","title":"3.1 Step 1 - Character Blueprints"},{"location":"ToolSummary/#4-tutorials_1","text":"","title":"4. Tutorials"},{"location":"TrafficManager/","text":"Traffic Manager Version Developer 1. Summary This module is designed to control the vehicles and traffic in simulation. This modules ensures that vehicles in simulation follows traffic rules. This modules manuvers the vehicles in simulation without any collision. 2. Method When the simulation starts first of all spawn points are calculate as per seed value on provided XODR. Once spawn points are generated vehicles stored in asset libraray are randomly spawned on points. After spawning a vehilcle it is initalised by its current route, default speed, haltDistanceFromFrontVehicle, tailgateDistance, emergencyHaltDistance, signalStoppingDistance , laneChangeProbability and boundingboxExtends. All the parameter are calculated randomly by gaussin distribution over mean and deviation value of respective variable. After all vehicles are iniitalised they starts manuvering on their current route. Now to control vehicles and to make them follow traffic rules different functions are callled, for example to control vehicles speed, Speed control function is called which controls the speed to vehicles speed according to vehilce position and position of other vehicles. There is Light stage funcion which ensures that vehicles follow traffic light rule. There is Collsion stage function which manages the junctions that has no traffic lights, it insures that there should no collision between the vehicles when passing through this junction. Then there is Lane change stage function which is responsible to make a vehicles lane change without any collision. These all function follows some rules to perforn all the actions. The rules for every function is listed below- Spawining of vehicles 1. The total number of spawnable points is calculated for a map based on a variable average distance $d$ with a random offset. 2. Each spawn point is located at a distance of $d$ (currently set at 15m) from the starting point of the road. 3. Spawn points are determined only within the driving lanes. 4. Lanes with a width less than 2m will not generate any spawn points. 5. The heading of spawn points is set according to the rule of the road. 6. Spawn points are not generated within junctions. 7. Spawnable points are shuffled using a random seed provided by the user. Thus, for a given seed, the spawn points will be deterministic. For perfect randomization, the seed \"time\" can be used. 8. The first $n$ spawn points are selected from the available spawnable points based on the number of vehicles specified.. 9. For any given road point, a lane is randomly chosen. Additionally, no two spawn points will lie parallel on lanes. 10. If the number of vehicles is greater than the available spawn points on the map, then the simulation stops. In other words, the number of vehicles must be less than or equal to the number of spawn points available. Speed Control 1. Vehicle speed is adjusted accordingly when anticipating a turn or approaching a junction. 2. At junctions, the maximum speed of vehicles is set to 20 km/h, while at turns it's calculated using the formulae: $\\sqrt(\\mu * g / \\left|\\text{Curvature}\\right|) * 3.6) - 6.0$ where $ \\mu $ is cofficient of friction $g$ is acceleration due to gravity, $curvature$ is the curvature of road at that point. 3. If a vehicle is traveling at a lower speed in front, the following vehicle adjusts its speed to maintain a safe tailgate distance between them. 4. Vehicles in junctions are not considered as vehicles in front; it's assumed that junction-specific rules apply, and collisions are avoided. 5. When encountering a stopped vehicle ahead, the vehicle stops before reaching a minimum distance from the stopped vehicle. 6. In the presence of any obstruction ahead, other than a moving vehicle in the lane (e.g., a horizontally parked vehicle), an emergency brake is applied. 7. The vehicle's speed is decreased by a decleration of $\\frac{{v^2 - u^2}}{{2d}}$, where where $v$ is the final speed and $u$ is the initial speed, over a distance $d$. Lights Stage 1. At a traffic light junction, vehicles follows the traffic light signals. 2. If the light is red or yellow, the vehicle stops at a given distance from the traffic light. 3. Only the first vehicle in the lane stops by the influence of the traffic light, subsequent vehicles stop because the vehicle in front is stopped. 4. The vehicle only starts moving when the light is green. 5. Once the vehicle is inside the junction, it\u2019ll continue moving even if the light has switched to red. This is to avoid the deadlock. 6. Vehicle senses traffic light not with a camera or any kind of perception but rather it mathematically calculates the distance and checks the heading between the vehicle and the light. Condition: Distance < 100m / Light Influence Distance && they must face each other && vehicle must come before the light in this facing. Collsion Stage 1. Multiple vehicles are allowed in the junction simultaneously. 2. Every vehicle in the junction checks its collision with other vehicles at the junction. 3. Vehicles that cannot collide with each other are allowed to pass through the junction. 4. Vehicles passing through the junction are randomly selected from a set of non-colliding vehicles. 5. Once a vehicle enters the junction it must continue moving without stopping within the junction. 6. It\u2019ll cause a deadlock if a vehicle for some reason fails inside the junction. Lane Chane Stage 1. Vehicle will only change lanes if the destination lane is a driving lane and its width is greater than 2m. 2. Vehicles don't change lanes while in a junction. 3. If Vehicle has both the left lane or right to choose, it chooses randomly between the left lane and right lane. 4. Every vehicle calculates its safe lane change distance based on their speed. The estimate is very crude and calculated by trial and error. 5. Based on safe distance vehicles see its back and front and check if lane change is safe or not, if safe then only do lane change. 6. If a junction is occuring in the safe distance checking it\u2019ll reject the lane change. 7. All the checking before the start of the maneuver. Once the lane change is started, no additional checking is done and it can\u2019t be stopped in between. This may lead to a scenario where a vehicle is blocking both the lanes because speed restriction came into play. To circumvent this, a persistent checking can be done which can cancel the maneuver in between if deemed so. 8. If lane change is assigned then, its current waypoints are scrubbed and a list of waypoints is generated starting the safe distance in the changing lane. PIDs are responsible for maneuvering and straightening. 9. Every time a lane change happens a small offset gets applied to the vehicle i.e. if the vehicle was moving in the center then after changing the lane it\u2019ll stabilize a little off the center. Traffic Manager General Rules 1. If an obstacle, such as a parked vehicle, obstructs the corner of a lane, approaching vehicles will come to a complete stop rather than attempting to navigate around it with a slight turn. 2. If a vehicle comes to a stop in the middle of the road due to a failure or for any other reason, approaching vehicles will also stop rather than overtaking the stationary vehicle. 3. Any vehicle out from the road for more than 1 second will be destroyed and removed from the map. Traffic Manager attributes Input attribute Type Default Description vehicleCount int 20 Number of vehicles to spawn. randomSeed int 1 . defaultSpeedMean float 60.0 Mean Default speed of vehicles. defaultSpeedDeviation float 5.0 Deviation from the default mean speed. haltDistanceFromFrontVehicleMean float 5.0 Mean halt distance from the front vehicle. haltDistanceFromFrontVehicleDeviation float 1.0 Deviation from the mean halt distance tailgateDistanceMean float 10.0 Mean tailgate distance. tailgateDistanceDeviation float 1.0 Deviation from the mean tailgate distance. emergencyHaltDistanceMean float 2.0 Mean emergency halt distance from the front vehicle. emergencyHaltDistanceDeviation float 0.5 Deviation from the mean emergency halt distance signalStoppingDistanceMean float 7.0 Mean stopping distance from the signal. signalStoppingDistanceDeviation float 1.0 Deviation from the mean stopping distance from the signal. laneChangeProbabilityMean float 0.002 Mean lane change probability. laneChangeProbabilityDeviation float 0.0001 Deviation from the mean lane change probability. maxThrottle float 0.85 Maximum value of vehicles throttle. [0,1] maxBrake float 0.3 Maximum value of vehicles brake. [0,1] maxSteer float 0.8 Maximum value of vehicles steer. [0,1] baseMinDistance float 2.0 Minimum distance for forward waypoint. 3. Requirements Unreal Engine","title":"Traffic"},{"location":"TrafficManager/#traffic-manager","text":"","title":"Traffic Manager"},{"location":"TrafficManager/#version","text":"","title":"Version"},{"location":"TrafficManager/#developer","text":"","title":"Developer"},{"location":"TrafficManager/#1-summary","text":"This module is designed to control the vehicles and traffic in simulation. This modules ensures that vehicles in simulation follows traffic rules. This modules manuvers the vehicles in simulation without any collision.","title":"1. Summary"},{"location":"TrafficManager/#2-method","text":"When the simulation starts first of all spawn points are calculate as per seed value on provided XODR. Once spawn points are generated vehicles stored in asset libraray are randomly spawned on points. After spawning a vehilcle it is initalised by its current route, default speed, haltDistanceFromFrontVehicle, tailgateDistance, emergencyHaltDistance, signalStoppingDistance , laneChangeProbability and boundingboxExtends. All the parameter are calculated randomly by gaussin distribution over mean and deviation value of respective variable. After all vehicles are iniitalised they starts manuvering on their current route. Now to control vehicles and to make them follow traffic rules different functions are callled, for example to control vehicles speed, Speed control function is called which controls the speed to vehicles speed according to vehilce position and position of other vehicles. There is Light stage funcion which ensures that vehicles follow traffic light rule. There is Collsion stage function which manages the junctions that has no traffic lights, it insures that there should no collision between the vehicles when passing through this junction. Then there is Lane change stage function which is responsible to make a vehicles lane change without any collision. These all function follows some rules to perforn all the actions. The rules for every function is listed below- Spawining of vehicles 1. The total number of spawnable points is calculated for a map based on a variable average distance $d$ with a random offset. 2. Each spawn point is located at a distance of $d$ (currently set at 15m) from the starting point of the road. 3. Spawn points are determined only within the driving lanes. 4. Lanes with a width less than 2m will not generate any spawn points. 5. The heading of spawn points is set according to the rule of the road. 6. Spawn points are not generated within junctions. 7. Spawnable points are shuffled using a random seed provided by the user. Thus, for a given seed, the spawn points will be deterministic. For perfect randomization, the seed \"time\" can be used. 8. The first $n$ spawn points are selected from the available spawnable points based on the number of vehicles specified.. 9. For any given road point, a lane is randomly chosen. Additionally, no two spawn points will lie parallel on lanes. 10. If the number of vehicles is greater than the available spawn points on the map, then the simulation stops. In other words, the number of vehicles must be less than or equal to the number of spawn points available. Speed Control 1. Vehicle speed is adjusted accordingly when anticipating a turn or approaching a junction. 2. At junctions, the maximum speed of vehicles is set to 20 km/h, while at turns it's calculated using the formulae: $\\sqrt(\\mu * g / \\left|\\text{Curvature}\\right|) * 3.6) - 6.0$ where $ \\mu $ is cofficient of friction $g$ is acceleration due to gravity, $curvature$ is the curvature of road at that point. 3. If a vehicle is traveling at a lower speed in front, the following vehicle adjusts its speed to maintain a safe tailgate distance between them. 4. Vehicles in junctions are not considered as vehicles in front; it's assumed that junction-specific rules apply, and collisions are avoided. 5. When encountering a stopped vehicle ahead, the vehicle stops before reaching a minimum distance from the stopped vehicle. 6. In the presence of any obstruction ahead, other than a moving vehicle in the lane (e.g., a horizontally parked vehicle), an emergency brake is applied. 7. The vehicle's speed is decreased by a decleration of $\\frac{{v^2 - u^2}}{{2d}}$, where where $v$ is the final speed and $u$ is the initial speed, over a distance $d$. Lights Stage 1. At a traffic light junction, vehicles follows the traffic light signals. 2. If the light is red or yellow, the vehicle stops at a given distance from the traffic light. 3. Only the first vehicle in the lane stops by the influence of the traffic light, subsequent vehicles stop because the vehicle in front is stopped. 4. The vehicle only starts moving when the light is green. 5. Once the vehicle is inside the junction, it\u2019ll continue moving even if the light has switched to red. This is to avoid the deadlock. 6. Vehicle senses traffic light not with a camera or any kind of perception but rather it mathematically calculates the distance and checks the heading between the vehicle and the light. Condition: Distance < 100m / Light Influence Distance && they must face each other && vehicle must come before the light in this facing. Collsion Stage 1. Multiple vehicles are allowed in the junction simultaneously. 2. Every vehicle in the junction checks its collision with other vehicles at the junction. 3. Vehicles that cannot collide with each other are allowed to pass through the junction. 4. Vehicles passing through the junction are randomly selected from a set of non-colliding vehicles. 5. Once a vehicle enters the junction it must continue moving without stopping within the junction. 6. It\u2019ll cause a deadlock if a vehicle for some reason fails inside the junction. Lane Chane Stage 1. Vehicle will only change lanes if the destination lane is a driving lane and its width is greater than 2m. 2. Vehicles don't change lanes while in a junction. 3. If Vehicle has both the left lane or right to choose, it chooses randomly between the left lane and right lane. 4. Every vehicle calculates its safe lane change distance based on their speed. The estimate is very crude and calculated by trial and error. 5. Based on safe distance vehicles see its back and front and check if lane change is safe or not, if safe then only do lane change. 6. If a junction is occuring in the safe distance checking it\u2019ll reject the lane change. 7. All the checking before the start of the maneuver. Once the lane change is started, no additional checking is done and it can\u2019t be stopped in between. This may lead to a scenario where a vehicle is blocking both the lanes because speed restriction came into play. To circumvent this, a persistent checking can be done which can cancel the maneuver in between if deemed so. 8. If lane change is assigned then, its current waypoints are scrubbed and a list of waypoints is generated starting the safe distance in the changing lane. PIDs are responsible for maneuvering and straightening. 9. Every time a lane change happens a small offset gets applied to the vehicle i.e. if the vehicle was moving in the center then after changing the lane it\u2019ll stabilize a little off the center. Traffic Manager General Rules 1. If an obstacle, such as a parked vehicle, obstructs the corner of a lane, approaching vehicles will come to a complete stop rather than attempting to navigate around it with a slight turn. 2. If a vehicle comes to a stop in the middle of the road due to a failure or for any other reason, approaching vehicles will also stop rather than overtaking the stationary vehicle. 3. Any vehicle out from the road for more than 1 second will be destroyed and removed from the map.","title":"2. Method"},{"location":"TrafficManager/#traffic-manager-attributes","text":"Input attribute Type Default Description vehicleCount int 20 Number of vehicles to spawn. randomSeed int 1 . defaultSpeedMean float 60.0 Mean Default speed of vehicles. defaultSpeedDeviation float 5.0 Deviation from the default mean speed. haltDistanceFromFrontVehicleMean float 5.0 Mean halt distance from the front vehicle. haltDistanceFromFrontVehicleDeviation float 1.0 Deviation from the mean halt distance tailgateDistanceMean float 10.0 Mean tailgate distance. tailgateDistanceDeviation float 1.0 Deviation from the mean tailgate distance. emergencyHaltDistanceMean float 2.0 Mean emergency halt distance from the front vehicle. emergencyHaltDistanceDeviation float 0.5 Deviation from the mean emergency halt distance signalStoppingDistanceMean float 7.0 Mean stopping distance from the signal. signalStoppingDistanceDeviation float 1.0 Deviation from the mean stopping distance from the signal. laneChangeProbabilityMean float 0.002 Mean lane change probability. laneChangeProbabilityDeviation float 0.0001 Deviation from the mean lane change probability. maxThrottle float 0.85 Maximum value of vehicles throttle. [0,1] maxBrake float 0.3 Maximum value of vehicles brake. [0,1] maxSteer float 0.8 Maximum value of vehicles steer. [0,1] baseMinDistance float 2.0 Minimum distance for forward waypoint.","title":"Traffic Manager attributes"},{"location":"TrafficManager/#3-requirements","text":"Unreal Engine","title":"3. Requirements"},{"location":"Util/","text":"Util Version Developer 1. Summary This module is designed to assist other modules, such as lidar and camera functionalities, within LimulatorUE. Additionally, it helps in logging, writing JSON files, and performing various functions regularly utilized in LimulatorUE, such as unit conversions specific to Unreal Engine and obtaining file locations within the project. 2. Method Util module consisits five class ImageUtil , LidarUtil , JSONUtil , LimulatorHelper , LimulatorLogger . ImageUtil is used by Image data worker to convert binary image data captured by camera sensor into JPG or PNG format. 1. ConvertToPng : Convert binary image data to PNG format. 2. ConvertToJpg : Convert binary image data to JPG format. 3. SaveToFile : Save binary image data to input file. 4. SavePngFile : First converts binary data to PNG format and then save to file. 5. SaveJpgFile : First converts binary data to JPG format and then save to file. The LidatUtil module consists of various functions used by lidar sensor: 1. GetMaterialNameByHitResult : This function returns the material of hit point resulting from ray casting. 2. GetIncidentAngle : This function returns the incident angle of lidar rays upon hitting a surface. 3. ComputeIntensityRotatingLidar : This function calculates the intensity for a Lidar sensor using the formula provided in the documentation for Lidar Sensor . 4. ComputeIntensitySolidStateLidar : This function calcultes the intensity for a Solid state Lidar sensor using the formula provided in the documentation for Lidar Sensor . 5. GetClassTagByHitResult : This function returns the class of the hit point resulting from ray casting. 6. AddRandomNoise : This function adds noise to the resulting hit points. The LimulatorHelper module consists of various helper functions designed to help in different tasks: 1. ComputeAngleBetweenTwoVectors : This function calculates the angle in degrees between two vectors. 2. VectorUnrealToNormal : It converts an Unreal Engine vector to a typical vector format used outside of Unreal. 3. VectorNormalToUnreal : This function converts a typical vector to an Unreal Engine vector. 4. RotatorNormalToUnreal : It converts rotations in the usual format to Unreal Engine rotations. 5. LocationToPosition : This function converts an Unreal Engine location to a roamanager::position. 6. PositionToFVector : It converts a roamanager::position to an Unreal Engine location vector. 7. GetXoscFilePath : This function returns the path of the input Xosc file. 8. GetXodrFilePath : This function returns the path of the input Xodr file. JSONUtil is used by bounding box sensor to save bounding box information into JSON file. LimulatorLogger is used to log anything in Limulator category. 3. Requirements Unreal Engine","title":"Util"},{"location":"Util/#util","text":"","title":"Util"},{"location":"Util/#version","text":"","title":"Version"},{"location":"Util/#developer","text":"","title":"Developer"},{"location":"Util/#1-summary","text":"This module is designed to assist other modules, such as lidar and camera functionalities, within LimulatorUE. Additionally, it helps in logging, writing JSON files, and performing various functions regularly utilized in LimulatorUE, such as unit conversions specific to Unreal Engine and obtaining file locations within the project.","title":"1. Summary"},{"location":"Util/#2-method","text":"Util module consisits five class ImageUtil , LidarUtil , JSONUtil , LimulatorHelper , LimulatorLogger . ImageUtil is used by Image data worker to convert binary image data captured by camera sensor into JPG or PNG format. 1. ConvertToPng : Convert binary image data to PNG format. 2. ConvertToJpg : Convert binary image data to JPG format. 3. SaveToFile : Save binary image data to input file. 4. SavePngFile : First converts binary data to PNG format and then save to file. 5. SaveJpgFile : First converts binary data to JPG format and then save to file. The LidatUtil module consists of various functions used by lidar sensor: 1. GetMaterialNameByHitResult : This function returns the material of hit point resulting from ray casting. 2. GetIncidentAngle : This function returns the incident angle of lidar rays upon hitting a surface. 3. ComputeIntensityRotatingLidar : This function calculates the intensity for a Lidar sensor using the formula provided in the documentation for Lidar Sensor . 4. ComputeIntensitySolidStateLidar : This function calcultes the intensity for a Solid state Lidar sensor using the formula provided in the documentation for Lidar Sensor . 5. GetClassTagByHitResult : This function returns the class of the hit point resulting from ray casting. 6. AddRandomNoise : This function adds noise to the resulting hit points. The LimulatorHelper module consists of various helper functions designed to help in different tasks: 1. ComputeAngleBetweenTwoVectors : This function calculates the angle in degrees between two vectors. 2. VectorUnrealToNormal : It converts an Unreal Engine vector to a typical vector format used outside of Unreal. 3. VectorNormalToUnreal : This function converts a typical vector to an Unreal Engine vector. 4. RotatorNormalToUnreal : It converts rotations in the usual format to Unreal Engine rotations. 5. LocationToPosition : This function converts an Unreal Engine location to a roamanager::position. 6. PositionToFVector : It converts a roamanager::position to an Unreal Engine location vector. 7. GetXoscFilePath : This function returns the path of the input Xosc file. 8. GetXodrFilePath : This function returns the path of the input Xodr file. JSONUtil is used by bounding box sensor to save bounding box information into JSON file. LimulatorLogger is used to log anything in Limulator category.","title":"2. Method"},{"location":"Util/#3-requirements","text":"Unreal Engine","title":"3. Requirements"},{"location":"VariationEngine/","text":"Variation Engine Version 1.0 Developer: Marut Priyadarshi 1. Summary This module takes OpenScenario files as input and, using a parameter reference database of supported parameters, generates variations of the input xosc file, one at a time or in bulk. 2. Method 2.1 Inputs The user makes a number of selections before executing the module: 1. Selecting the OpenScenario file, multiple files or a folder containing multiple OpenScenario files. 2. Selecting from a list of available options, the parameters which will be permuted when generating variations. 3. Selecting the type of distribution logic for the permutation of numeric parameters. + Dense: The parameter is uniformly distributed in the range of +/-10% of the original parameter value. + Sparse: The parameter is uniformly distributed in the maximum range defined in the reference database for that category of parameter. 2.2 Parameter Search Based on the user input, the module searches the xosc file for the selected parameters. There are different functions for searching and handling the parameters based on their datatype and values: 1. speedSearch() : This searches for instances of initial speed parameters of vehicles. 2. bodyChange() : This searches for vehicle model parameters. 3. accelChange() : This searches for the acceleration parameters. 4. decelChange() : This searches for deceleration parameters. 5. laneChangeSpeed() : This searches for the parameter which defines the duration of the lane change action. When the module encounters an instance of the selected parameters, it calls the permuteNum() or permuteStr() function to generate permutations. Then it continues the search for the next instance of the selected parameters. 2.3 Parameter Reference We have created a database which contains the supported parameters on which permutations can be performed, the permissible ranges for those parameters (eg: speed, acceleration) and different range of values for categories of vehicles. Currently, different categories of vehicles (cars, trucks) get generic values for their cateegory parameters. Future support for different values for each vehicle model is planned. This database is referenced by the permutation module while generating variations to define the ranges for the numerical valued parameters. 2.4 Permutor: After encountering an instance of the parameter in the search, the module cross-references it with the Parameter Reference and depending on the distribution logic input by the user, it generates variations of the scenario by permuting the current parameter. The function used is dependent on the datatype of the parameter 1. permuteNum() handles the permutation of numeric valued parameters (integer, float, double) 2. permuteStr() handles the permutation of string valued parameters. Before completing an execution cycle, the function resets the parameter value to its original value before giving the control back to the parameter search module. 2.5 File Writer: The permutor module calls the write() module to continously write the generated permutations into new xosc files, generating the variations of the input xosc file(s) 3. Requirements Python 3.10","title":"Variation Engine"},{"location":"VariationEngine/#variation-engine","text":"","title":"Variation Engine"},{"location":"VariationEngine/#version-10","text":"","title":"Version 1.0"},{"location":"VariationEngine/#developer-marut-priyadarshi","text":"","title":"Developer: Marut Priyadarshi"},{"location":"VariationEngine/#1-summary","text":"This module takes OpenScenario files as input and, using a parameter reference database of supported parameters, generates variations of the input xosc file, one at a time or in bulk.","title":"1. Summary"},{"location":"VariationEngine/#2-method","text":"","title":"2. Method"},{"location":"VariationEngine/#21-inputs","text":"The user makes a number of selections before executing the module: 1. Selecting the OpenScenario file, multiple files or a folder containing multiple OpenScenario files. 2. Selecting from a list of available options, the parameters which will be permuted when generating variations. 3. Selecting the type of distribution logic for the permutation of numeric parameters. + Dense: The parameter is uniformly distributed in the range of +/-10% of the original parameter value. + Sparse: The parameter is uniformly distributed in the maximum range defined in the reference database for that category of parameter.","title":"2.1 Inputs"},{"location":"VariationEngine/#22-parameter-search","text":"Based on the user input, the module searches the xosc file for the selected parameters. There are different functions for searching and handling the parameters based on their datatype and values: 1. speedSearch() : This searches for instances of initial speed parameters of vehicles. 2. bodyChange() : This searches for vehicle model parameters. 3. accelChange() : This searches for the acceleration parameters. 4. decelChange() : This searches for deceleration parameters. 5. laneChangeSpeed() : This searches for the parameter which defines the duration of the lane change action. When the module encounters an instance of the selected parameters, it calls the permuteNum() or permuteStr() function to generate permutations. Then it continues the search for the next instance of the selected parameters.","title":"2.2 Parameter Search"},{"location":"VariationEngine/#23-parameter-reference","text":"We have created a database which contains the supported parameters on which permutations can be performed, the permissible ranges for those parameters (eg: speed, acceleration) and different range of values for categories of vehicles. Currently, different categories of vehicles (cars, trucks) get generic values for their cateegory parameters. Future support for different values for each vehicle model is planned. This database is referenced by the permutation module while generating variations to define the ranges for the numerical valued parameters.","title":"2.3 Parameter Reference"},{"location":"VariationEngine/#24-permutor","text":"After encountering an instance of the parameter in the search, the module cross-references it with the Parameter Reference and depending on the distribution logic input by the user, it generates variations of the scenario by permuting the current parameter. The function used is dependent on the datatype of the parameter 1. permuteNum() handles the permutation of numeric valued parameters (integer, float, double) 2. permuteStr() handles the permutation of string valued parameters. Before completing an execution cycle, the function resets the parameter value to its original value before giving the control back to the parameter search module.","title":"2.4 Permutor:"},{"location":"VariationEngine/#25-file-writer","text":"The permutor module calls the write() module to continously write the generated permutations into new xosc files, generating the variations of the input xosc file(s)","title":"2.5 File Writer:"},{"location":"VariationEngine/#3-requirements","text":"Python 3.10","title":"3. Requirements"},{"location":"VehiclesModelling/","text":"Vehicle Blueprints Version 1.0 Modeler - Manasvi Kale 1. Summary This document helps you create a Vehicle blueprint equipped with vehicle lights which are in alignment with vehicle controller. 2. Method Vehicle Blueprint deriving from \"limulatorwheeledvehicle\" base class inherits all the base properties and functions of a vehicle. Vehicle lights are placed at relevant positions with relevant intensity values. These Blueprints are spawned in raod map at run time in simulation. Refer How-to-guide to set up all the required compoents for vehicle blueprints. flowchart - 3. Requirements Vehicle Skeletal Mesh AnimationBP for Vehicles Front and Rear Tyre Blueprints LimulatorWheeledVehicle Base class SimpleLimulatorWheeledVehicle Base class 4. How-to-guide 4.1 Step 1 - Creating Vehicle Blueprint Step 1 - Create BP and inherit it from \u201cLimulatorWheeledVehicleBase\u201d and name it as per vehicle model. Step 2 - Add the vehicle skeletal mesh and relevant animation BP to it. Step 3 - Select Vehicle Movement Component. 3.1 Inside vehicle set up heading add mass. 3.2 In the Wheel setups add front and rear wheels and wheel class. 3.2 Add chassis width and chassis height as per your car model. Step 4 - All the inherited lights are to be adjusted at their respective places as per the model. (i.e headlights, brake lights, indicator etc) The radius for lights will change for each vehicle model. Adjust that as well. Step 5 - Copy all the blueprint functions from the base class blueprint and paste it in the new BP. Step 6 - Open event graph of the vehicle. Step 7 - Add \u201cEvent refresh lights\u201d node and add refresh vehicle lights and connect them. Also join \u201cin lights state\u201d to \u201cvehicle lights state\u201d. Step 8 - Apply and save. Vehicles created with \"LimulatorWheeledVehicleBase\" are used simulation in pre-built maps. Steps for creating vehicles for XOSC pipeline are all same mentioned above. The base class however changes to \"SimpleLimulatorWheeledVehicle\". 5. Tutorial","title":"Vehicles"},{"location":"VehiclesModelling/#vehicle-blueprints","text":"","title":"Vehicle Blueprints"},{"location":"VehiclesModelling/#version-10","text":"","title":"Version 1.0"},{"location":"VehiclesModelling/#modeler-manasvi-kale","text":"","title":"Modeler - Manasvi Kale"},{"location":"VehiclesModelling/#1-summary","text":"This document helps you create a Vehicle blueprint equipped with vehicle lights which are in alignment with vehicle controller.","title":"1. Summary"},{"location":"VehiclesModelling/#2-method","text":"Vehicle Blueprint deriving from \"limulatorwheeledvehicle\" base class inherits all the base properties and functions of a vehicle. Vehicle lights are placed at relevant positions with relevant intensity values. These Blueprints are spawned in raod map at run time in simulation. Refer How-to-guide to set up all the required compoents for vehicle blueprints. flowchart -","title":"2. Method"},{"location":"VehiclesModelling/#3-requirements","text":"Vehicle Skeletal Mesh AnimationBP for Vehicles Front and Rear Tyre Blueprints LimulatorWheeledVehicle Base class SimpleLimulatorWheeledVehicle Base class","title":"3. Requirements"},{"location":"VehiclesModelling/#4-how-to-guide","text":"","title":"4. How-to-guide"},{"location":"VehiclesModelling/#41-step-1-creating-vehicle-blueprint","text":"Step 1 - Create BP and inherit it from \u201cLimulatorWheeledVehicleBase\u201d and name it as per vehicle model. Step 2 - Add the vehicle skeletal mesh and relevant animation BP to it. Step 3 - Select Vehicle Movement Component. 3.1 Inside vehicle set up heading add mass. 3.2 In the Wheel setups add front and rear wheels and wheel class. 3.2 Add chassis width and chassis height as per your car model. Step 4 - All the inherited lights are to be adjusted at their respective places as per the model. (i.e headlights, brake lights, indicator etc) The radius for lights will change for each vehicle model. Adjust that as well. Step 5 - Copy all the blueprint functions from the base class blueprint and paste it in the new BP. Step 6 - Open event graph of the vehicle. Step 7 - Add \u201cEvent refresh lights\u201d node and add refresh vehicle lights and connect them. Also join \u201cin lights state\u201d to \u201cvehicle lights state\u201d. Step 8 - Apply and save. Vehicles created with \"LimulatorWheeledVehicleBase\" are used simulation in pre-built maps. Steps for creating vehicles for XOSC pipeline are all same mentioned above. The base class however changes to \"SimpleLimulatorWheeledVehicle\".","title":"4.1 Step 1 - Creating Vehicle Blueprint"},{"location":"VehiclesModelling/#5-tutorial","text":"","title":"5. Tutorial"},{"location":"core/","text":"Limulator Core Version 1.0 Developer 1. Summary Limulator Core is the library that handles the parsing and execution of OpenX standards. This is modules are precompiled and integrated with the LimulatorUE as a third-party library. It mainly consists of two parts: RoadManager and ScenarioEngine. RoadManager is designed for OpenDrive 1.4 and works alongside the Limulator map, providing essential high-level information about the map. ScenarioEngine parses OpenSCENARIO files and manages their execution. We use ScenarioEngine as a backend to run our OpenX simulations. The communication and integration are handled by the Scenario Handler OpenX, connecting with the Limulator Core and Limulator UE for smooth operation. 2. Method 2.1 Road Manager 2.2 Scenario Engine 2.3 3. Requirements","title":"Core"},{"location":"core/#limulator-core","text":"","title":"Limulator Core"},{"location":"core/#version-10","text":"","title":"Version 1.0"},{"location":"core/#developer","text":"","title":"Developer"},{"location":"core/#1-summary","text":"Limulator Core is the library that handles the parsing and execution of OpenX standards. This is modules are precompiled and integrated with the LimulatorUE as a third-party library. It mainly consists of two parts: RoadManager and ScenarioEngine. RoadManager is designed for OpenDrive 1.4 and works alongside the Limulator map, providing essential high-level information about the map. ScenarioEngine parses OpenSCENARIO files and manages their execution. We use ScenarioEngine as a backend to run our OpenX simulations. The communication and integration are handled by the Scenario Handler OpenX, connecting with the Limulator Core and Limulator UE for smooth operation.","title":"1. Summary"},{"location":"core/#2-method","text":"","title":"2. Method"},{"location":"core/#21-road-manager","text":"","title":"2.1 Road Manager"},{"location":"core/#22-scenario-engine","text":"","title":"2.2 Scenario Engine"},{"location":"core/#23","text":"","title":"2.3"},{"location":"core/#3-requirements","text":"","title":"3. Requirements"},{"location":"game/","text":"Game 1. Summary The Game module is responsible for overseeing the entire simulation, serving as a central coordinator for other modules. It extends global support to all modules, facilitating seamless coordination. Acting akin to a manager overseeing a team, the Game module assigns tasks to other modules and captures their outputs, ensuring effective collaboration and functioning across the simulation. 2. Method 2.1 Scenario Handlers Scenario Handlers define an abstract class for handling different sensor data generation pipelines. Briefly, it attaches sensor to ego, manages environment and handles ticking of the simulation. 2.1.1 OpenX Handler OpenX handler is responsible for simulating OpenX scenarios. This handlers acts as a bridge between the core and the simulation, updating entity position and adding the sensors. 2.1.2 MapFlow Handler Mapflow handler generates stochastic simulation using the auto traffic functionality over custom files. Inputs to this are the map HD file and configuration for auto traffic. The aim for this pipeline is to generate a random city like traffic simulaiton. Note: Simulation using auto-traffic can't be resimulated exactly even when given the same seed. 2.1.3 LevelFlow Handler This is quite similar to MapFlow scenario. It's differentiates MapFlow scenario with two things: it adds functionality of intelligent pedestrians in the simulation, it runs of pre-made level which includes with HD Map an static 3D environment. Note: Simulation using auto-traffic can't be resimulated exactly even when given the same seed. 2.2 Parser Parser is responsible for reading the configuration file given to the limulator. It reads the file, checks for correctness and finds the appropriate scenario handler to be used. 2.3 Annotator Annotator is reposible for annotating all the actors in the evironment or newly spawned actors. This annoation is of great help in the semantic segmentation. 2.4 Actor Factories Actors factories follow the simple factory design. All objects spawned in the simulation must go through the actor factory. This helps us in making sure that all objects are annotated and initializations are done properly. 2.5 Blueprint Registries Blueprint Registry brings the bluerprints created in asset library for use in our limulator. Every vehicle, pedestrian, or other dynamic must be added to the registry. 2.6 Level Opener & Game Modes Every simulation has certain fundamentals, like controller entity (in our case they're scenario handlers), transition between levels, etc. These fundamental concepts and rules are defined by GameMode. We call our custom Game mode LimulatorUEGameModeBase. Note: This is the entry point after starting the game. It handles the creation of parser and then spawning the approriate scenario handler. After that, it passes control to the scenario handlers.","title":"Game"},{"location":"game/#game","text":"","title":"Game"},{"location":"game/#1-summary","text":"The Game module is responsible for overseeing the entire simulation, serving as a central coordinator for other modules. It extends global support to all modules, facilitating seamless coordination. Acting akin to a manager overseeing a team, the Game module assigns tasks to other modules and captures their outputs, ensuring effective collaboration and functioning across the simulation.","title":"1. Summary"},{"location":"game/#2-method","text":"","title":"2. Method"},{"location":"game/#21-scenario-handlers","text":"Scenario Handlers define an abstract class for handling different sensor data generation pipelines. Briefly, it attaches sensor to ego, manages environment and handles ticking of the simulation.","title":"2.1 Scenario Handlers"},{"location":"game/#211-openx-handler","text":"OpenX handler is responsible for simulating OpenX scenarios. This handlers acts as a bridge between the core and the simulation, updating entity position and adding the sensors.","title":"2.1.1 OpenX Handler"},{"location":"game/#212-mapflow-handler","text":"Mapflow handler generates stochastic simulation using the auto traffic functionality over custom files. Inputs to this are the map HD file and configuration for auto traffic. The aim for this pipeline is to generate a random city like traffic simulaiton. Note: Simulation using auto-traffic can't be resimulated exactly even when given the same seed.","title":"2.1.2 MapFlow Handler"},{"location":"game/#213-levelflow-handler","text":"This is quite similar to MapFlow scenario. It's differentiates MapFlow scenario with two things: it adds functionality of intelligent pedestrians in the simulation, it runs of pre-made level which includes with HD Map an static 3D environment. Note: Simulation using auto-traffic can't be resimulated exactly even when given the same seed.","title":"2.1.3 LevelFlow Handler"},{"location":"game/#22-parser","text":"Parser is responsible for reading the configuration file given to the limulator. It reads the file, checks for correctness and finds the appropriate scenario handler to be used.","title":"2.2 Parser"},{"location":"game/#23-annotator","text":"Annotator is reposible for annotating all the actors in the evironment or newly spawned actors. This annoation is of great help in the semantic segmentation.","title":"2.3 Annotator"},{"location":"game/#24-actor-factories","text":"Actors factories follow the simple factory design. All objects spawned in the simulation must go through the actor factory. This helps us in making sure that all objects are annotated and initializations are done properly.","title":"2.4 Actor Factories"},{"location":"game/#25-blueprint-registries","text":"Blueprint Registry brings the bluerprints created in asset library for use in our limulator. Every vehicle, pedestrian, or other dynamic must be added to the registry.","title":"2.5 Blueprint Registries"},{"location":"game/#26-level-opener-game-modes","text":"Every simulation has certain fundamentals, like controller entity (in our case they're scenario handlers), transition between levels, etc. These fundamental concepts and rules are defined by GameMode. We call our custom Game mode LimulatorUEGameModeBase. Note: This is the entry point after starting the game. It handles the creation of parser and then spawning the approriate scenario handler. After that, it passes control to the scenario handlers.","title":"2.6 Level Opener &amp; Game Modes"},{"location":"lights/","text":"Lights Version 1.0 Developer 1. Summary Lights module is responsible for controlling the behaviour and display of lights in the simulation environment. It is intergrated with the weather module in an observer pattern. Lights are autonomously controlled with weather. 2. Method 2.1 Street Lights All street lights in the simulation are derived from a common street light base class. It provides a simple interface which is mostly about turning on/off. 2.2 Stand-alone Traffic Lights Traffic lights are kept aside from their logic. It has two components: a traffic light class, which provides an interface of turning on/off. The interface is further implemented by the concrete blueprint classes. It's logic is encapsulated in controllers. Controllers control the current state of a traffic light and changes is appropriately. Two types of controllers are implemented -- for single lights and for a complete group at junctions. 2.3 Crossings Crossings form an important part of the traffic network. They are deeply integrated with the traffic lights to control the flow of pedestrians. Crossings have a state of their own which allows or rejects the flow of pedestrians. It's state is either always allowing or is governed by the attached traffic light. 4. How-to-guide How to add a new Street Light How to add a new Traffic Light How to add a new Crossing How to add a new Traffic Light Group Controller","title":"Lights"},{"location":"lights/#lights","text":"","title":"Lights"},{"location":"lights/#version-10","text":"","title":"Version 1.0"},{"location":"lights/#developer","text":"","title":"Developer"},{"location":"lights/#1-summary","text":"Lights module is responsible for controlling the behaviour and display of lights in the simulation environment. It is intergrated with the weather module in an observer pattern. Lights are autonomously controlled with weather.","title":"1. Summary"},{"location":"lights/#2-method","text":"","title":"2. Method"},{"location":"lights/#21-street-lights","text":"All street lights in the simulation are derived from a common street light base class. It provides a simple interface which is mostly about turning on/off.","title":"2.1 Street Lights"},{"location":"lights/#22-stand-alone-traffic-lights","text":"Traffic lights are kept aside from their logic. It has two components: a traffic light class, which provides an interface of turning on/off. The interface is further implemented by the concrete blueprint classes. It's logic is encapsulated in controllers. Controllers control the current state of a traffic light and changes is appropriately. Two types of controllers are implemented -- for single lights and for a complete group at junctions.","title":"2.2 Stand-alone Traffic Lights"},{"location":"lights/#23-crossings","text":"Crossings form an important part of the traffic network. They are deeply integrated with the traffic lights to control the flow of pedestrians. Crossings have a state of their own which allows or rejects the flow of pedestrians. It's state is either always allowing or is governed by the attached traffic light.","title":"2.3 Crossings"},{"location":"lights/#4-how-to-guide","text":"How to add a new Street Light How to add a new Traffic Light How to add a new Crossing How to add a new Traffic Light Group Controller","title":"4. How-to-guide"},{"location":"map/","text":"Map Version 1.0 Developer 1. Summary The Map module encapsulates the 3D model of the environment, incorporating a structured HD map for the road. The HD Map adheres to the OpenDrive standard (refer to the ASAM website for details). Additionally, the Map module offers various intelligent functionalities, including automated terrain generation, intelligent abstract queries on the HD Map, and navigation point generation. Limulator Map heavily relies on the road manager provided by esmini, which handles most of the substantial tasks. However, it also incorporates the additions done by the map creator (e.g. addional traffic signs, lights) and its own intelligent algorithms to process data supplied by the road manager. While Limulator Map is intricately woven into the Limulator project, an external shared library is available for querying the map, albeit with some limitations. There's also an autnomous terrain generator capable of generating a terrain that aligns with the road definition of HD Map. It is very useful in creating the illusion of real world while running OpenX simulations. 2. Method 2.1 Limulator Map 2.2 Terrain Generator 3. Requirements","title":"Map"},{"location":"map/#map","text":"","title":"Map"},{"location":"map/#version-10","text":"","title":"Version 1.0"},{"location":"map/#developer","text":"","title":"Developer"},{"location":"map/#1-summary","text":"The Map module encapsulates the 3D model of the environment, incorporating a structured HD map for the road. The HD Map adheres to the OpenDrive standard (refer to the ASAM website for details). Additionally, the Map module offers various intelligent functionalities, including automated terrain generation, intelligent abstract queries on the HD Map, and navigation point generation. Limulator Map heavily relies on the road manager provided by esmini, which handles most of the substantial tasks. However, it also incorporates the additions done by the map creator (e.g. addional traffic signs, lights) and its own intelligent algorithms to process data supplied by the road manager. While Limulator Map is intricately woven into the Limulator project, an external shared library is available for querying the map, albeit with some limitations. There's also an autnomous terrain generator capable of generating a terrain that aligns with the road definition of HD Map. It is very useful in creating the illusion of real world while running OpenX simulations.","title":"1. Summary"},{"location":"map/#2-method","text":"","title":"2. Method"},{"location":"map/#21-limulator-map","text":"","title":"2.1 Limulator Map"},{"location":"map/#22-terrain-generator","text":"","title":"2.2 Terrain Generator"},{"location":"map/#3-requirements","text":"","title":"3. Requirements"},{"location":"visualizer/","text":"Visualizer Version 1.0 Developer 1. Summary The Visualizer module is employed for the real-time visualization of sensor output data. It utilizes a TCP socket to manage incoming data and transmits it to the visualization window. The TCP server functionality is implemented based on an open-source project, ensuring robust handling of data transfer for effective visualization. 2. Method 2.1 Camera Visualization Data is sent from limulator in form of jpg byte stream. This byte stream is traferred using TCP Connection and images are displace with Python Imaging Library on a tkinter window. Images are constantly updated on windows depending upon the sensor frequency. 2.2 Lidar Visualization Data sent from the lidar sensor in from of numpy array file format bytestream. This is constanly updated in a double ended queue and diplayed on a openGL window. 3. Requirements","title":"Visalizer"},{"location":"visualizer/#visualizer","text":"","title":"Visualizer"},{"location":"visualizer/#version-10","text":"","title":"Version 1.0"},{"location":"visualizer/#developer","text":"","title":"Developer"},{"location":"visualizer/#1-summary","text":"The Visualizer module is employed for the real-time visualization of sensor output data. It utilizes a TCP socket to manage incoming data and transmits it to the visualization window. The TCP server functionality is implemented based on an open-source project, ensuring robust handling of data transfer for effective visualization.","title":"1. Summary"},{"location":"visualizer/#2-method","text":"","title":"2. Method"},{"location":"visualizer/#21-camera-visualization","text":"Data is sent from limulator in form of jpg byte stream. This byte stream is traferred using TCP Connection and images are displace with Python Imaging Library on a tkinter window. Images are constantly updated on windows depending upon the sensor frequency.","title":"2.1 Camera Visualization"},{"location":"visualizer/#22-lidar-visualization","text":"Data sent from the lidar sensor in from of numpy array file format bytestream. This is constanly updated in a double ended queue and diplayed on a openGL window.","title":"2.2 Lidar Visualization"},{"location":"visualizer/#3-requirements","text":"","title":"3. Requirements"},{"location":"weather/","text":"Weather Version 1.0 Developer 1. Summary The Weather module, as its name implies, is tasked with controlling the weather within the 3D environment. It manages elements such as sun position, fog, and precipitation, among other factors, to create a dynamic and realistic weather simulation. Weather module is mainly implemented in blueprints. See BP_Sky and BP_Wether for more. 2. Method 2.1 BP_Sky BP_Sky is the module that control the static environment and required DirectionalLight and Skylight to work. It can change the material applied on the planet mesh, sun postion, etc among other things. It provides interface for the following: 1. Sun Altitude 2. Sun Azimuth 3. Cloudiness 4. Fog BP_Sky is not meant to be directly controlled by the user. It is used as a componenet by BP_Weather. 2.2 BP_Weather BP_Weather is mainly responsible for controlling the dynamic weather in the environment. BP_Weather depends on BP_Sky and will not work without it. It provides and interface to change the following: 1. Cloudiness 2. Precipitation 3. Precipitation Deposits 4. Wind 5. Sun Altitude 6. Sun Azimuth 7. Fog 8. Wetness 9. Scattering Intensity 10. Dust Note: Weather module is highly inspired from the weather module in CARLA. 3. Requirements 4. How-to-guide 4.1 Step 1 Step 1 - 1 Step 1 - 2 Step 1 - 3 4.2 Step 2 Step 2 - 1 Step 2 - 2 Step 2 - 3 4.3 Step 3 Step 3 - 1 Step 3 - 2 Step 3 - 3 5. Tutorials","title":"Weather"},{"location":"weather/#weather","text":"","title":"Weather"},{"location":"weather/#version-10","text":"","title":"Version 1.0"},{"location":"weather/#developer","text":"","title":"Developer"},{"location":"weather/#1-summary","text":"The Weather module, as its name implies, is tasked with controlling the weather within the 3D environment. It manages elements such as sun position, fog, and precipitation, among other factors, to create a dynamic and realistic weather simulation. Weather module is mainly implemented in blueprints. See BP_Sky and BP_Wether for more.","title":"1. Summary"},{"location":"weather/#2-method","text":"","title":"2. Method"},{"location":"weather/#21-bp_sky","text":"BP_Sky is the module that control the static environment and required DirectionalLight and Skylight to work. It can change the material applied on the planet mesh, sun postion, etc among other things. It provides interface for the following: 1. Sun Altitude 2. Sun Azimuth 3. Cloudiness 4. Fog BP_Sky is not meant to be directly controlled by the user. It is used as a componenet by BP_Weather.","title":"2.1 BP_Sky"},{"location":"weather/#22-bp_weather","text":"BP_Weather is mainly responsible for controlling the dynamic weather in the environment. BP_Weather depends on BP_Sky and will not work without it. It provides and interface to change the following: 1. Cloudiness 2. Precipitation 3. Precipitation Deposits 4. Wind 5. Sun Altitude 6. Sun Azimuth 7. Fog 8. Wetness 9. Scattering Intensity 10. Dust Note: Weather module is highly inspired from the weather module in CARLA.","title":"2.2 BP_Weather"},{"location":"weather/#3-requirements","text":"","title":"3. Requirements"},{"location":"weather/#4-how-to-guide","text":"","title":"4. How-to-guide"},{"location":"weather/#41-step-1","text":"Step 1 - 1 Step 1 - 2 Step 1 - 3","title":"4.1 Step 1"},{"location":"weather/#42-step-2","text":"Step 2 - 1 Step 2 - 2 Step 2 - 3","title":"4.2 Step 2"},{"location":"weather/#43-step-3","text":"Step 3 - 1 Step 3 - 2 Step 3 - 3","title":"4.3 Step 3"},{"location":"weather/#5-tutorials","text":"","title":"5. Tutorials"}]}